# Tutorial III: Howtos

### 1) How to freeze a module during training like in Keras/Lasagne?
To *freeze* a module during training, use the `include` and `exclude` arguments of module's `.collect_params()` and `.collect_self_updates()` functions.
#### Example
```python
class FOO(Module):
    def __init__(self):
        self.cnn0 = Conv2D(...)
        self.cnn1 = Conv2D(...)
        self.cnn2 = Conv2D(...)
        ....
        
# Now we will freeze cnn0 and cnn1 submodules during training
model    = Foo()
loss     = ...
params   = model.collect_params(exclude=['cnn0', 'cnn1'])
updates  = optimizer(loss, params)
updates.update(model.colect_self_updates(exclude=['cnn0', 'cnn1']))
train_fn = theano.function([...], [...], updates=updates, no_default_updates=False)
```

### 2) How to add random noise to a tensor?
Just use Theano's `MRG_RandomStreams` module.
#### Example
```python
from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams
srng = RandomStreams(np.random.randint(1, 2147462579))
....
y = x + srng.normal(x.shape, avg=0.0, std=0.1)   # add Gaussian noise to x
```
What you'd keep in mind is that if you used Theano's `MRG_RandomStreams` module, remember to set `no_default_updates=False` when compiling functions.

### 3) How to do model-parallel training?
According to [issue 6655](https://github.com/Theano/Theano/issues/6655), model-parallel multi-GPU support of Theano will never be finished, so it won't be possible to do model-parallel training with Theano, and of course, Dandelion.  
For data-parallel training, refer to [platoon](https://github.com/mila-udem/platoon) for possible solution. We may implement our multi-GPU data-parallel training scheme later, stay tuned.