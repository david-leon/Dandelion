{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dandelion A quite light weight deep learning framework, on top of Theano, offering better balance between flexibility and abstraction. Targeted Users Researchers who need flexibility as well as convenience to experiment all kinds of nonstandard network structures, and also the stability of Theano. Why Another DL Framework The reason is more about the lack of flexibility for existing DL frameworks, such as Keras, Lasagne, Blocks, etc. By \u201cflexibility\u201d , we means whether it is easy to modify or extend the framework. The famous DL framework Keras is designed to be beginner-friendly oriented, at the cost of being quite hard to modify. Compared to Keras, another less-famous framework Lasagne provides more flexibility. It\u2019s easier to write your own layer by Lasagne for small neural network, however, for complex neural networks it still needs quite manual works because like other existing frameworks, Lasagne operates on abstracted \u2018Layer\u2019 class instead of raw tensor variables. Featuring Aiming to offer better balance between flexibility and abstraction. Easy to use and extend, support for any neural network structure. Loose coupling, each part of the framework can be modified independently. More like a handy library of deep learning modules. Common modules such as CNN, LSTM, GRU, Dense, Dropout, Batch Normalization, and common optimization methods such as SGD, Adam, Adadelta, Rmsprop are ready out-of-the-box. Plug & play, operating directly on Theano tensors, no upper abstraction applied. Unlike previous frameworks like Keras, Lasagne, etc., Dandelion operates directly on tensors instead of layer abstractions, making it quite easy to plug in 3rd part defined deep learning modules (layer defined by Keras/Lasagne) or vice versa. Project Layout Python Module Explanation module all neual network module definitions functional operations on tensor with no parameter to be learned initialization initialization methods for neural network modules activation definition of all activation functions objective definition of all loss objectives update definition of all optimizers util utility functions model model implementations out-of-the-box ext extensions Credits The design of Dandelion heavily draws on Lasagne and Pytorch , both my favorate DL libraries. Special Thanks To Radomir Dopieralski , who transferred the dandelion project name on pypi to us. Now you can install the package by simply pip install dandelion .","title":"Home"},{"location":"#dandelion","text":"A quite light weight deep learning framework, on top of Theano, offering better balance between flexibility and abstraction.","title":"Dandelion"},{"location":"#targeted-users","text":"Researchers who need flexibility as well as convenience to experiment all kinds of nonstandard network structures, and also the stability of Theano.","title":"Targeted Users"},{"location":"#why-another-dl-framework","text":"The reason is more about the lack of flexibility for existing DL frameworks, such as Keras, Lasagne, Blocks, etc. By \u201cflexibility\u201d , we means whether it is easy to modify or extend the framework. The famous DL framework Keras is designed to be beginner-friendly oriented, at the cost of being quite hard to modify. Compared to Keras, another less-famous framework Lasagne provides more flexibility. It\u2019s easier to write your own layer by Lasagne for small neural network, however, for complex neural networks it still needs quite manual works because like other existing frameworks, Lasagne operates on abstracted \u2018Layer\u2019 class instead of raw tensor variables.","title":"Why Another DL Framework"},{"location":"#featuring","text":"Aiming to offer better balance between flexibility and abstraction. Easy to use and extend, support for any neural network structure. Loose coupling, each part of the framework can be modified independently. More like a handy library of deep learning modules. Common modules such as CNN, LSTM, GRU, Dense, Dropout, Batch Normalization, and common optimization methods such as SGD, Adam, Adadelta, Rmsprop are ready out-of-the-box. Plug & play, operating directly on Theano tensors, no upper abstraction applied. Unlike previous frameworks like Keras, Lasagne, etc., Dandelion operates directly on tensors instead of layer abstractions, making it quite easy to plug in 3rd part defined deep learning modules (layer defined by Keras/Lasagne) or vice versa.","title":"Featuring"},{"location":"#project-layout","text":"Python Module Explanation module all neual network module definitions functional operations on tensor with no parameter to be learned initialization initialization methods for neural network modules activation definition of all activation functions objective definition of all loss objectives update definition of all optimizers util utility functions model model implementations out-of-the-box ext extensions","title":"Project Layout"},{"location":"#credits","text":"The design of Dandelion heavily draws on Lasagne and Pytorch , both my favorate DL libraries.","title":"Credits"},{"location":"#special-thanks","text":"To Radomir Dopieralski , who transferred the dandelion project name on pypi to us. Now you can install the package by simply pip install dandelion .","title":"Special Thanks"},{"location":"dandelion_activation/","text":"Dandelion's activation module is mostly inherited from Lasagne except for the softmax() and log_softmax() activations. You're recommended to refer to Lasagne.nonlinearities document for the following activations: sigmoid tanh relu softplus ultra_fast_sigmoid ScaledTanH leaky_rectify very_leaky_rectify elu SELU linear identity softmax Apply softmax to the last dimension of input x softmax(x) x : theano tensor of any shape log_softmax Apply softmax to the last dimension of input x , in log domain log_softmax(x) x : theano tensor of any shape","title":"dandelion.activation"},{"location":"dandelion_activation/#softmax","text":"Apply softmax to the last dimension of input x softmax(x) x : theano tensor of any shape","title":"softmax"},{"location":"dandelion_activation/#log_softmax","text":"Apply softmax to the last dimension of input x , in log domain log_softmax(x) x : theano tensor of any shape","title":"log_softmax"},{"location":"dandelion_ext_CV/","text":"Image Processing and Computer Vision Toolkits imread Read image file and return as numpy ndarray , using PILLOW as backend. Support for EXIF rotation specification. imread(f, flatten=False, dtype='float32') f : str or file object. The file name or file object to be read from. flatten : bool. If True , flattens the color channels into a single gray-scale channel. dtype : returned data type imsave Save an image ndarray into file, using PILLOW as backend imsave(f, I, **params) f : str or file object. The file name or file object to be written into. I : Image ndarray . Note for jpeg format, I should be of uint8 type. params : other parameters passed directly to PILLOW's image.save() imresize Resize image, using scipy as backend imresize(I, size, interp='bilinear', mode=None) I : Image ndarray size : target size interp : Interpolation to use for resizing, {'nearest', 'lanczos', 'bilinear', 'bicubic' or 'cubic'}. mode : . The PIL image mode ('P', 'L', etc.) to convert I before resizing, optional. imrotate Rotate image, using opencv as backend imrotate(I, angle, padvalue=0.0, interpolation='linear', target_size=None, border_mode='reflect_101') I : Image ndarray angle : in degree, positive for counter-clockwise interpolation : image interpolation method, {'linear'|'nearest'|'cubic'|'LANCZOS4'|'area'}, refer to opencv:INTER_* constants for details border_mode : image boundary handling method, {'reflect_101'|'reflect'|'wrap'|'constant'|'replicate'}, refer to opencv:BORDER_* constants for details padvalue : used when border_mode = 'constant' target_size : target size of output image, optional.","title":"dandelion.ext.CV"},{"location":"dandelion_ext_CV/#imread","text":"Read image file and return as numpy ndarray , using PILLOW as backend. Support for EXIF rotation specification. imread(f, flatten=False, dtype='float32') f : str or file object. The file name or file object to be read from. flatten : bool. If True , flattens the color channels into a single gray-scale channel. dtype : returned data type","title":"imread"},{"location":"dandelion_ext_CV/#imsave","text":"Save an image ndarray into file, using PILLOW as backend imsave(f, I, **params) f : str or file object. The file name or file object to be written into. I : Image ndarray . Note for jpeg format, I should be of uint8 type. params : other parameters passed directly to PILLOW's image.save()","title":"imsave"},{"location":"dandelion_ext_CV/#imresize","text":"Resize image, using scipy as backend imresize(I, size, interp='bilinear', mode=None) I : Image ndarray size : target size interp : Interpolation to use for resizing, {'nearest', 'lanczos', 'bilinear', 'bicubic' or 'cubic'}. mode : . The PIL image mode ('P', 'L', etc.) to convert I before resizing, optional.","title":"imresize"},{"location":"dandelion_ext_CV/#imrotate","text":"Rotate image, using opencv as backend imrotate(I, angle, padvalue=0.0, interpolation='linear', target_size=None, border_mode='reflect_101') I : Image ndarray angle : in degree, positive for counter-clockwise interpolation : image interpolation method, {'linear'|'nearest'|'cubic'|'LANCZOS4'|'area'}, refer to opencv:INTER_* constants for details border_mode : image boundary handling method, {'reflect_101'|'reflect'|'wrap'|'constant'|'replicate'}, refer to opencv:BORDER_* constants for details padvalue : used when border_mode = 'constant' target_size : target size of output image, optional.","title":"imrotate"},{"location":"dandelion_ext_visual/","text":"Model Summary and Visualization Toolkits get_model_size Calculate model parameter size, return result in bytes. get_model_size(model) model : model defined by Dandelion get_model_summary Produce model parameter summary. get_model_summary(model, size_unit='M') model : model defined by Dandelion size_unit : { 'M' |'K'|'B'|int}, unit for calculating parameter size. return : OrderedDict instance. You can use json.dumps() to get a formatted json report file. For example: import json from dandelion.model import Alternate_2D_LSTM input_dim, hidden_dim, B, H, W = 8, 8, 2, 32, 32 model = Alternate_2D_LSTM(input_dims=[input_dim],hidden_dim=hidden_dim, peephole=True, mode=2) model_summary = get_model_summary(model, size_unit=1) rpt = json.dumps(model_summary, ensure_ascii=False, indent=2) print(rpt) The following is a snapshot of a complex model's summary, in which : the size attribute is in MB unit. the percent attribute is level-wise, and already in [0 ~ 100] range. For example in the snapshot, it says model_6 (weights) is 21.36MB in total, and the first convolution layer stage1 accounts for 0.011% of all the weights.","title":"dandelion.ext.visual"},{"location":"dandelion_ext_visual/#get_model_size","text":"Calculate model parameter size, return result in bytes. get_model_size(model) model : model defined by Dandelion","title":"get_model_size"},{"location":"dandelion_ext_visual/#get_model_summary","text":"Produce model parameter summary. get_model_summary(model, size_unit='M') model : model defined by Dandelion size_unit : { 'M' |'K'|'B'|int}, unit for calculating parameter size. return : OrderedDict instance. You can use json.dumps() to get a formatted json report file. For example: import json from dandelion.model import Alternate_2D_LSTM input_dim, hidden_dim, B, H, W = 8, 8, 2, 32, 32 model = Alternate_2D_LSTM(input_dims=[input_dim],hidden_dim=hidden_dim, peephole=True, mode=2) model_summary = get_model_summary(model, size_unit=1) rpt = json.dumps(model_summary, ensure_ascii=False, indent=2) print(rpt) The following is a snapshot of a complex model's summary, in which : the size attribute is in MB unit. the percent attribute is level-wise, and already in [0 ~ 100] range. For example in the snapshot, it says model_6 (weights) is 21.36MB in total, and the first convolution layer stage1 accounts for 0.011% of all the weights.","title":"get_model_summary"},{"location":"dandelion_functional/","text":"pool_1d Pooling 1 dimension along the given axis, support for any dimensional input. pool_1d(x, ws=2, ignore_border=True, stride=None, pad=0, mode='max', axis=-1) ws : scalar int. Factor by which to downsample the input ignore_border : bool. When True , dimension size=5 with ws =2 will generate a dimension size=2 output. 3 otherwise. stride : scalar int. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions), eg: stride =1 will shifts over one row for every iteration. pad : pad zeros to extend beyond border of the input mode : { max , sum , average_inc_pad , average_exc_pad }. Operation executed on each window. max and sum always exclude the padding in the computation. average gives you the choice to include or exclude it. axis : scalar int. Specify along which axis the pooling will be done pool_2d Pooling 2 dimension along the last 2 dimensions of input, support for any dimensional input with ndim >=2. pool_2d(x, ws=(2,2), ignore_border=True, stride=None, pad=(0,0), mode='max') ws : scalar tuple. Factor by which to downsample the input ignore_border : bool. When True , (5,5) input with ws =(2,2) will generate a (2,2) output. (3,3) otherwise. stride : scalar tuple. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions), eg: stride =(1,1) will shifts over one row and one column for every iteration. pad : pad zeros to extend beyond border of the input mode : { max , sum , average_inc_pad , average_exc_pad }. Operation executed on each window. max and sum always exclude the padding in the computation. average gives you the choice to include or exclude it. pool_3d Pooling 3 dimension along the last 3 dimensions of input, support for any dimensional input with ndim >=3. pool_3d(x, ws=(2,2,2), ignore_border=True, stride=None, pad=(0,0,0), mode='max') ws : scalar tuple. Factor by which to downsample the input ignore_border : bool. When True , (5,5,5) input with ws =(2,2,2) will generate a (2,2,2) output. (3,3,3) otherwise. stride : scalar tuple. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions). pad : pad zeros to extend beyond border of the input mode : { max , sum , average_inc_pad , average_exc_pad }. Operation executed on each window. max and sum always exclude the padding in the computation. average gives you the choice to include or exclude it. align_crop Align a list of tensors at each axis by specified rules and crop them to make axis concatenation possible. align_crop(tensor_list, cropping) tensor_list : list of tensors to be processed, they much have the same ndim s. cropping : list of cropping rules for each dimension. Acceptable rules include { None | lower | upper | center }. None : this axis is not cropped, tensors are unchanged in this axis lower : tensors are cropped choosing the lower portion in this axis as a[:crop_size, ...] upper : tensors are cropped choosing the upper portion in this axis as a[-crop_size:, ...] center : tensors are cropped choosing the central portion in this axis as a[offset:offset+crop_size, ...] where offset = (a.shape[0]-crop_size)//2) spatial_pyramid_pooling Spatial pyramid pooling. This function will use different scale pooling pyramid to generate spatially fix-sized output no matter the spatial size of input, useful when CNN+FC used for image classification or detection with variable-sized samples. spatial_pyramid_pooling(x, pyramid_dims=(6, 4, 2, 1), mode='max', implementation='fast') x : 4D tensor with shape (B, C, H, W) pyramid_dims : list or tuple of integers. Refer to Ref[1] for details. mode : { max , sum , average_inc_pad , average_exc_pad }. Operation executed on each window. max and sum always exclude the padding in the computation. average gives you the choice to include or exclude it. implementation : { fast , fast_ls , stretch }. fast : The 'fast' implementation is fast and pad zero when input size is too small. fast_ls : The 'fast_ls' implementation is same as Lasagne fast implementation. The size of the input map MUST be larger than the output map size. stretch : The 'stretch' implementation is slower. The implementation will get same feature at some position just like nearest neighbor interpolation when the input size is less than the output size. Ref [1]: He, Kaiming et al (2015), Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. http://arxiv.org/pdf/1406.4729.pdf upsample_2d Upsample 2 dimension along the last 2 dimensions of input, support for any dimensional input with ndim >=2. Only integer upsampling ratio supported. upsample_2d(x, ratio, mode='repeat') ratio : ust be integer or tuple of integers >=1 mode : { repeat , dilate }. Repeat element values or upsample leaving zeroes between upsampled elements. Default repeat . upsample_2d_bilinear Upsample 2D with bilinear interpolation. Support for fractional ratio, and only apply for 4D tensor. upsample_2d_bilinear(x, ratio=None, frac_ratio=None, use_1D_kernel=True) ratio : ust be integer or tuple of integers >=1. You can only specify either ratio or frac_ratio , not both. frac_ratio : None, tuple of int or tuple of tuples of int. A fractional upsampling scale is described by (numerator, denominator). use_1D_kernel : only for speed matter. Note: due to Theano's implementation, when the upsampling ratio is even, the last row and column is repeated one extra time compared to the first row and column which makes the upsampled tensor asymmetrical on both sides. This does not happen when the upsampling ratio is odd. channel_shuffle Pseudo shuffling channel by dimshuffle & reshape, first introduced in ShuffleNet channel_shuffle(x, group_num) x : 4D tensor, with shape (B, C, H, W) , usually output of a 2D convolution. group_num : int scalar, and C must be divisible by group_num","title":"dandelion.functional"},{"location":"dandelion_functional/#pool_1d","text":"Pooling 1 dimension along the given axis, support for any dimensional input. pool_1d(x, ws=2, ignore_border=True, stride=None, pad=0, mode='max', axis=-1) ws : scalar int. Factor by which to downsample the input ignore_border : bool. When True , dimension size=5 with ws =2 will generate a dimension size=2 output. 3 otherwise. stride : scalar int. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions), eg: stride =1 will shifts over one row for every iteration. pad : pad zeros to extend beyond border of the input mode : { max , sum , average_inc_pad , average_exc_pad }. Operation executed on each window. max and sum always exclude the padding in the computation. average gives you the choice to include or exclude it. axis : scalar int. Specify along which axis the pooling will be done","title":"pool_1d"},{"location":"dandelion_functional/#pool_2d","text":"Pooling 2 dimension along the last 2 dimensions of input, support for any dimensional input with ndim >=2. pool_2d(x, ws=(2,2), ignore_border=True, stride=None, pad=(0,0), mode='max') ws : scalar tuple. Factor by which to downsample the input ignore_border : bool. When True , (5,5) input with ws =(2,2) will generate a (2,2) output. (3,3) otherwise. stride : scalar tuple. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions), eg: stride =(1,1) will shifts over one row and one column for every iteration. pad : pad zeros to extend beyond border of the input mode : { max , sum , average_inc_pad , average_exc_pad }. Operation executed on each window. max and sum always exclude the padding in the computation. average gives you the choice to include or exclude it.","title":"pool_2d"},{"location":"dandelion_functional/#pool_3d","text":"Pooling 3 dimension along the last 3 dimensions of input, support for any dimensional input with ndim >=3. pool_3d(x, ws=(2,2,2), ignore_border=True, stride=None, pad=(0,0,0), mode='max') ws : scalar tuple. Factor by which to downsample the input ignore_border : bool. When True , (5,5,5) input with ws =(2,2,2) will generate a (2,2,2) output. (3,3,3) otherwise. stride : scalar tuple. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions). pad : pad zeros to extend beyond border of the input mode : { max , sum , average_inc_pad , average_exc_pad }. Operation executed on each window. max and sum always exclude the padding in the computation. average gives you the choice to include or exclude it.","title":"pool_3d"},{"location":"dandelion_functional/#align_crop","text":"Align a list of tensors at each axis by specified rules and crop them to make axis concatenation possible. align_crop(tensor_list, cropping) tensor_list : list of tensors to be processed, they much have the same ndim s. cropping : list of cropping rules for each dimension. Acceptable rules include { None | lower | upper | center }. None : this axis is not cropped, tensors are unchanged in this axis lower : tensors are cropped choosing the lower portion in this axis as a[:crop_size, ...] upper : tensors are cropped choosing the upper portion in this axis as a[-crop_size:, ...] center : tensors are cropped choosing the central portion in this axis as a[offset:offset+crop_size, ...] where offset = (a.shape[0]-crop_size)//2)","title":"align_crop"},{"location":"dandelion_functional/#spatial_pyramid_pooling","text":"Spatial pyramid pooling. This function will use different scale pooling pyramid to generate spatially fix-sized output no matter the spatial size of input, useful when CNN+FC used for image classification or detection with variable-sized samples. spatial_pyramid_pooling(x, pyramid_dims=(6, 4, 2, 1), mode='max', implementation='fast') x : 4D tensor with shape (B, C, H, W) pyramid_dims : list or tuple of integers. Refer to Ref[1] for details. mode : { max , sum , average_inc_pad , average_exc_pad }. Operation executed on each window. max and sum always exclude the padding in the computation. average gives you the choice to include or exclude it. implementation : { fast , fast_ls , stretch }. fast : The 'fast' implementation is fast and pad zero when input size is too small. fast_ls : The 'fast_ls' implementation is same as Lasagne fast implementation. The size of the input map MUST be larger than the output map size. stretch : The 'stretch' implementation is slower. The implementation will get same feature at some position just like nearest neighbor interpolation when the input size is less than the output size. Ref [1]: He, Kaiming et al (2015), Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. http://arxiv.org/pdf/1406.4729.pdf","title":"spatial_pyramid_pooling"},{"location":"dandelion_functional/#upsample_2d","text":"Upsample 2 dimension along the last 2 dimensions of input, support for any dimensional input with ndim >=2. Only integer upsampling ratio supported. upsample_2d(x, ratio, mode='repeat') ratio : ust be integer or tuple of integers >=1 mode : { repeat , dilate }. Repeat element values or upsample leaving zeroes between upsampled elements. Default repeat .","title":"upsample_2d"},{"location":"dandelion_functional/#upsample_2d_bilinear","text":"Upsample 2D with bilinear interpolation. Support for fractional ratio, and only apply for 4D tensor. upsample_2d_bilinear(x, ratio=None, frac_ratio=None, use_1D_kernel=True) ratio : ust be integer or tuple of integers >=1. You can only specify either ratio or frac_ratio , not both. frac_ratio : None, tuple of int or tuple of tuples of int. A fractional upsampling scale is described by (numerator, denominator). use_1D_kernel : only for speed matter. Note: due to Theano's implementation, when the upsampling ratio is even, the last row and column is repeated one extra time compared to the first row and column which makes the upsampled tensor asymmetrical on both sides. This does not happen when the upsampling ratio is odd.","title":"upsample_2d_bilinear"},{"location":"dandelion_functional/#channel_shuffle","text":"Pseudo shuffling channel by dimshuffle & reshape, first introduced in ShuffleNet channel_shuffle(x, group_num) x : 4D tensor, with shape (B, C, H, W) , usually output of a 2D convolution. group_num : int scalar, and C must be divisible by group_num","title":"channel_shuffle"},{"location":"dandelion_initialization/","text":"Dandelion's initialization module is mostly inherited from Lasagne . You're recommended to refer to Lasagne.init document for the details.","title":"dandelion.initialization"},{"location":"dandelion_model/","text":"VGG-16 network Reference implementation of the classic VGG-16 network class model_VGG16(channel=3, im_height=224, im_width=224, Nclass=1000, kernel_size=3, border_mode=(1, 1), flip_filters=False) channel : input channel number Nclass : output class number The model accepts input of shape in the order of (B, C, H, W), and outputs with shape (B, N). Depthwise Separable Convolution Reference implementation of Depthwise Separable Convolution class DSConv2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), dilation=(1,1), pad='valid') input_channels : int. Input shape is (B, input_channels, H_in, W_in) out_channels : int. Output shape is (B output_channels, H_out, W_out) kernel_size : int scalar or tuple of int. Convolution kernel size stride : Factor by which to subsample the output pad : same / valid / full or 2-element tuple of int. Control image border padding. dilation : factor by which to subsample (stride) the input. The model do the depthwise 2D convolution per-channel of input, then map the output to #out_channels number of channel by pointwise 1*1 convolution. No activation applied inside. ResNet bottleneck Reference implementation of bottleneck building block of ResNet network class ResNet_bottleneck(outer_channel=256, inner_channel=64, border_mode='same', batchnorm_mode=1, activation=relu) outer_channel : channel number of block input inner_channel : channel number inside the block batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last element-wise sum output. The model accepts input of shape in the order of (B, C, H, W), and outputs with the same shape. Feature Pyramid Network Reference implementation of feature pyramid network class model_FPN(input_channel=3, base_n_filters=64, batchnorm_mode=1) batchnorm_mode : same with ResNet_bottleneck return 4-element tuple (p2, p3, p4, p5) , CNN pyramid features at different scales, each with #channel = 4 * base_n_filters ShuffleUnit Reference implementation of shuffle-net unit class ShuffleUnit(in_channels=256, inner_channels=None, out_channels=None, group_num=4, border_mode='same', batchnorm_mode=1, activation=relu, stride=(1,1), dilation=(1,1), fusion_mode='add') in_channels : channel number of unit input inner_channel : optional, channel number inside the unit, default = in_channels//4 out_channels : channel number of unit output, only used when fusion_mode = 'concat', and must > in_channels group_num : number of convolution groups border_mode : only same allowed batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output. stride, dilation : only used for depthwise separable convolution module inside fusion_mode : {'add' | 'concat'}. When 'concat', out_channels must > in_channels . return : convolution result with #channel = in_channels when fusion_mode ='add', #channel = out_channels when fusion_mode ='concat' ShuffleUnit_Stack Reference implementation of shuffle-net unit stack class ShuffleUnit_Stack(in_channels, inner_channels=None, out_channels=None, group_num=4, batchnorm_mode=1, activation=relu, stack_size=3, stride=2, fusion_mode='concat') in_channels : channel number of input inner_channel : optional, channel number inside the shuffle-unit, default = in_channels//4 out_channels : channel number of stack output, must > in_channels group_num : number of convolution groups batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output. stack_size : number of shuffle-unit in the stack stride : int or tuple of int, convolution stride for the first unit, default=2 fusion_mode : fusion_mode for the first unit. ShuffleNet Reference implementation of shuffle-net , without the final pooling & Dense layer. class model_ShuffleNet(in_channels, group_num=4, stage_channels=(24, 272, 544, 1088), stack_size=(3, 7, 3), batchnorm_mode=1, activation=relu) in_channels : channel number of input group_num : number of convolution groups stage_channels : channel number of each stage output. stack_size : size of each stack. batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output. ShuffleUnit_v2 Reference implementation of shufflenet_v2 unit class ShuffleUnit_v2(in_channels=256, out_channels=None, border_mode='same', batchnorm_mode=1, activation=relu, stride=1, dilation=1) in_channels : channel number of unit input out_channels : channel number of unit output, only used when stride >1; when stride1 =1, out_channels is fixed to in_channels . border_mode : only same allowed batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output. stride, dilation : only used for depthwise separable convolution module inside, must be integer scalars or tuple of integers. ShuffleUnit_v2_Stack Reference implementation of shufflenet_v2 unit stack class ShuffleUnit_v2_Stack(in_channels, out_channels, batchnorm_mode=1, activation=relu, stack_size=3, stride=2) in_channels : channel number of input out_channels : channel number of stack output batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output. stack_size : number of shuffle-unit in the stack stride : int or tuple of int, convolution stride for the first unit, default=2 ShuffleNet_v2 Reference implementation of shufflenet_v2 , without the final pooling & Dense layer. class model_ShuffleNet_v2(in_channels, stage_channels=(24, 116, 232, 464, 1024), stack_size=(3, 7, 3), batchnorm_mode=1, activation=relu) in_channels : channel number of input stage_channels : channel number of each stage output. stack_size : size of each stack. batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output. CTPN Model reference implementation of CTPN class model_CTPN(k=10, do_side_refinement_regress=False, batchnorm_mode=1, channel=3, im_height=None, im_width=None, kernel_size=3, border_mode=(1, 1), VGG_flip_filters=False, im2col=None) k : anchor box number do_side_refinement_regress : whether implement side refinement regression batchnorm_mode : {0| 1 }, whether insert batch normalization into the end of each convolution stage of VGG-16 net, useful for cold start. channel : input channel number im_height, im_width : input image height/width, optional kernel_size : convolution kernel size of VGG-16 net border_mode : border mode of VGG-16 net VGG_flip_filters : whether flip convolution kernels for VGG-16 net im2col : function corresponding to Caffe's im2col() . If None , the CTPN implementation will not strictly follow the original paper. U-net FCN Reference implementation of U-net FCN class model_Unet(channel=1, im_height=128, im_width=128, Nclass=2, kernel_size=3, border_mode='same', base_n_filters=64, output_activation=softmax) channel : input channel number Nclass : output channel number The model accepts input of shape in the order of (B, C, H, W), and outputs with shape in the order of (B, H, W, C). Shuffle-Seg network Model reference implementation of ShuffleSeg class model_ShuffleSeg(in_channels=1, Nclass=6, SF_group_num=4, SF_stage_channels=(24, 272, 544, 1088), SF_stack_size=(3, 7, 3), SF_batchnorm_mode=1, SF_activation=relu) in_channels : channel number of input Nclass : output class number SF_group_num : number of convolution groups for inside ShuffleNet encoder. SF_stage_channels : channel number of each stage output for inside ShuffleNet encoder. SF_stack_size : size of each stack for inside ShuffleNet encoder. SF_batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn. For inside ShuffleNet encoder SF_activation : default = relu. For inside ShuffleNet encoder. Alternate 2D LSTM LSTM2D implementation by alternating LSTM along different dimensions. Input shape = (H, W, B, C) class Alternate_2D_LSTM( input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, mode=2) All the arguments are the same with LSTM module, except for mode . mode : {0 | 1 | 2 }. 0: concat mode, 1D LSTM results from horizontal and vertical dimensions are concatenated along the C dimension, i.e., result = concat(horizontal\\_LSTM(input), vertical\\_LSTM(input)) result = concat(horizontal\\_LSTM(input), vertical\\_LSTM(input)) ; 1: sequential mode, horizontal and vertical dimensions are processed sequentially, i.e., result = horizontal\\_LSTM(vertical\\_LSTM(input)) result = horizontal\\_LSTM(vertical\\_LSTM(input)) ; 2: mixed mode, i.e., result = horizontal\\_LSTM(concat(input, vertical\\_LSTM(input))) result = horizontal\\_LSTM(concat(input, vertical\\_LSTM(input))) .forward(seq_input, h_ini=(None, None), c_ini=(None, None), seq_mask=None, backward=(False, False), return_final_state=False) All the arguments are the same with LSTM module .predict = .forward","title":"dandelion.model"},{"location":"dandelion_model/#vgg-16-network","text":"Reference implementation of the classic VGG-16 network class model_VGG16(channel=3, im_height=224, im_width=224, Nclass=1000, kernel_size=3, border_mode=(1, 1), flip_filters=False) channel : input channel number Nclass : output class number The model accepts input of shape in the order of (B, C, H, W), and outputs with shape (B, N).","title":"VGG-16 network"},{"location":"dandelion_model/#depthwise-separable-convolution","text":"Reference implementation of Depthwise Separable Convolution class DSConv2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), dilation=(1,1), pad='valid') input_channels : int. Input shape is (B, input_channels, H_in, W_in) out_channels : int. Output shape is (B output_channels, H_out, W_out) kernel_size : int scalar or tuple of int. Convolution kernel size stride : Factor by which to subsample the output pad : same / valid / full or 2-element tuple of int. Control image border padding. dilation : factor by which to subsample (stride) the input. The model do the depthwise 2D convolution per-channel of input, then map the output to #out_channels number of channel by pointwise 1*1 convolution. No activation applied inside.","title":"Depthwise Separable Convolution"},{"location":"dandelion_model/#resnet-bottleneck","text":"Reference implementation of bottleneck building block of ResNet network class ResNet_bottleneck(outer_channel=256, inner_channel=64, border_mode='same', batchnorm_mode=1, activation=relu) outer_channel : channel number of block input inner_channel : channel number inside the block batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last element-wise sum output. The model accepts input of shape in the order of (B, C, H, W), and outputs with the same shape.","title":"ResNet bottleneck"},{"location":"dandelion_model/#feature-pyramid-network","text":"Reference implementation of feature pyramid network class model_FPN(input_channel=3, base_n_filters=64, batchnorm_mode=1) batchnorm_mode : same with ResNet_bottleneck return 4-element tuple (p2, p3, p4, p5) , CNN pyramid features at different scales, each with #channel = 4 * base_n_filters","title":"Feature Pyramid Network"},{"location":"dandelion_model/#shuffleunit","text":"Reference implementation of shuffle-net unit class ShuffleUnit(in_channels=256, inner_channels=None, out_channels=None, group_num=4, border_mode='same', batchnorm_mode=1, activation=relu, stride=(1,1), dilation=(1,1), fusion_mode='add') in_channels : channel number of unit input inner_channel : optional, channel number inside the unit, default = in_channels//4 out_channels : channel number of unit output, only used when fusion_mode = 'concat', and must > in_channels group_num : number of convolution groups border_mode : only same allowed batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output. stride, dilation : only used for depthwise separable convolution module inside fusion_mode : {'add' | 'concat'}. When 'concat', out_channels must > in_channels . return : convolution result with #channel = in_channels when fusion_mode ='add', #channel = out_channels when fusion_mode ='concat'","title":"ShuffleUnit"},{"location":"dandelion_model/#shuffleunit_stack","text":"Reference implementation of shuffle-net unit stack class ShuffleUnit_Stack(in_channels, inner_channels=None, out_channels=None, group_num=4, batchnorm_mode=1, activation=relu, stack_size=3, stride=2, fusion_mode='concat') in_channels : channel number of input inner_channel : optional, channel number inside the shuffle-unit, default = in_channels//4 out_channels : channel number of stack output, must > in_channels group_num : number of convolution groups batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output. stack_size : number of shuffle-unit in the stack stride : int or tuple of int, convolution stride for the first unit, default=2 fusion_mode : fusion_mode for the first unit.","title":"ShuffleUnit_Stack"},{"location":"dandelion_model/#shufflenet","text":"Reference implementation of shuffle-net , without the final pooling & Dense layer. class model_ShuffleNet(in_channels, group_num=4, stage_channels=(24, 272, 544, 1088), stack_size=(3, 7, 3), batchnorm_mode=1, activation=relu) in_channels : channel number of input group_num : number of convolution groups stage_channels : channel number of each stage output. stack_size : size of each stack. batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output.","title":"ShuffleNet"},{"location":"dandelion_model/#shuffleunit_v2","text":"Reference implementation of shufflenet_v2 unit class ShuffleUnit_v2(in_channels=256, out_channels=None, border_mode='same', batchnorm_mode=1, activation=relu, stride=1, dilation=1) in_channels : channel number of unit input out_channels : channel number of unit output, only used when stride >1; when stride1 =1, out_channels is fixed to in_channels . border_mode : only same allowed batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output. stride, dilation : only used for depthwise separable convolution module inside, must be integer scalars or tuple of integers.","title":"ShuffleUnit_v2"},{"location":"dandelion_model/#shuffleunit_v2_stack","text":"Reference implementation of shufflenet_v2 unit stack class ShuffleUnit_v2_Stack(in_channels, out_channels, batchnorm_mode=1, activation=relu, stack_size=3, stride=2) in_channels : channel number of input out_channels : channel number of stack output batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output. stack_size : number of shuffle-unit in the stack stride : int or tuple of int, convolution stride for the first unit, default=2","title":"ShuffleUnit_v2_Stack"},{"location":"dandelion_model/#shufflenet_v2","text":"Reference implementation of shufflenet_v2 , without the final pooling & Dense layer. class model_ShuffleNet_v2(in_channels, stage_channels=(24, 116, 232, 464, 1024), stack_size=(3, 7, 3), batchnorm_mode=1, activation=relu) in_channels : channel number of input stage_channels : channel number of each stage output. stack_size : size of each stack. batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn activation : default = relu. Note no activation applied to the last output.","title":"ShuffleNet_v2"},{"location":"dandelion_model/#ctpn","text":"Model reference implementation of CTPN class model_CTPN(k=10, do_side_refinement_regress=False, batchnorm_mode=1, channel=3, im_height=None, im_width=None, kernel_size=3, border_mode=(1, 1), VGG_flip_filters=False, im2col=None) k : anchor box number do_side_refinement_regress : whether implement side refinement regression batchnorm_mode : {0| 1 }, whether insert batch normalization into the end of each convolution stage of VGG-16 net, useful for cold start. channel : input channel number im_height, im_width : input image height/width, optional kernel_size : convolution kernel size of VGG-16 net border_mode : border mode of VGG-16 net VGG_flip_filters : whether flip convolution kernels for VGG-16 net im2col : function corresponding to Caffe's im2col() . If None , the CTPN implementation will not strictly follow the original paper.","title":"CTPN"},{"location":"dandelion_model/#u-net-fcn","text":"Reference implementation of U-net FCN class model_Unet(channel=1, im_height=128, im_width=128, Nclass=2, kernel_size=3, border_mode='same', base_n_filters=64, output_activation=softmax) channel : input channel number Nclass : output channel number The model accepts input of shape in the order of (B, C, H, W), and outputs with shape in the order of (B, H, W, C).","title":"U-net FCN"},{"location":"dandelion_model/#shuffle-seg-network","text":"Model reference implementation of ShuffleSeg class model_ShuffleSeg(in_channels=1, Nclass=6, SF_group_num=4, SF_stage_channels=(24, 272, 544, 1088), SF_stack_size=(3, 7, 3), SF_batchnorm_mode=1, SF_activation=relu) in_channels : channel number of input Nclass : output class number SF_group_num : number of convolution groups for inside ShuffleNet encoder. SF_stage_channels : channel number of each stage output for inside ShuffleNet encoder. SF_stack_size : size of each stack for inside ShuffleNet encoder. SF_batchnorm_mode : {0 | 1 | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn. For inside ShuffleNet encoder SF_activation : default = relu. For inside ShuffleNet encoder.","title":"Shuffle-Seg network"},{"location":"dandelion_model/#alternate-2d-lstm","text":"LSTM2D implementation by alternating LSTM along different dimensions. Input shape = (H, W, B, C) class Alternate_2D_LSTM( input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, mode=2) All the arguments are the same with LSTM module, except for mode . mode : {0 | 1 | 2 }. 0: concat mode, 1D LSTM results from horizontal and vertical dimensions are concatenated along the C dimension, i.e., result = concat(horizontal\\_LSTM(input), vertical\\_LSTM(input)) result = concat(horizontal\\_LSTM(input), vertical\\_LSTM(input)) ; 1: sequential mode, horizontal and vertical dimensions are processed sequentially, i.e., result = horizontal\\_LSTM(vertical\\_LSTM(input)) result = horizontal\\_LSTM(vertical\\_LSTM(input)) ; 2: mixed mode, i.e., result = horizontal\\_LSTM(concat(input, vertical\\_LSTM(input))) result = horizontal\\_LSTM(concat(input, vertical\\_LSTM(input))) .forward(seq_input, h_ini=(None, None), c_ini=(None, None), seq_mask=None, backward=(False, False), return_final_state=False) All the arguments are the same with LSTM module .predict = .forward","title":"Alternate 2D LSTM"},{"location":"dandelion_module/","text":"Module Root class of all network modules, you'd always subclass this for a new module class Module(name=None, work_mode='inference') name : module name, optional. If you don't specify the module name, it will be auto-named if this module is a sub-module of another module. work_mode : working mode, optional. Only used for the unified calling interface, check \"Tutorial I\" for detailed explanation. .params = [] .self_updating_variables = [] .sub_modules = OrderedDict() .name = name .work_mode = work_mode params : contains all the parameters which should be updated by optimizer (submodule excluded) self_updating_variables : contains all the parameters which are updated by user specified expression (submodule excluded) sub_modules : contains all the sub-modules .register_param(x, shape=None, name=None) .register_self_updating_variable(x, shape=None, name=None) Register and possibly initialize a parameter tensor. Parameters to be updated by optimizer should be registered with register_param() meanwhile parameters self-updated should be registerd with register_self_updating_variable() x : Theano shared variable, expression, numpy array or callable. Initial value, expression or initializer for this parameter. shape : tuple of int, optional. A tuple of integers representing the desired shape of the parameter tensor. name : str, optional. It's recommended to let the Dandelion framework name the variable automatically. .collect_params(include=None, exclude=None, include_self=True) Collect parameters to be updated by optimizer. include : sub-module keys, means which sub-module to include exclude : sub-module keys, means which sub-module to exclude include_self : whether include self.params return : list of parameters, in the same order of sub-modules .collect_self_updates(include=None, exclude=None, include_self=True) Collect all update from self_updating_variables. include : sub-module keys, means which sub-module to include exclude : sub-module keys, means which sub-module to exclude include_self : whether include self.self_updating_variables return : update dict, in the same order of sub-modules .get_weights() Collect all module weights (including submodules) return : list of tuples with format [variable.value, variable.name] .set_weights(module_weights, check_name='ignore') Set module weights by default order (same order with .get_weights() ) module_weights : same with the return of .get_weights() check_name : ignore | warn | raise . What to do if a weight's name does not match its corresponding variable's name. .set_weights_by_name(module_weights, unmatched='raise') Set module weights by matching name. module_weights : same with the return of .get_weights() unmatched : ignore | warn | raise . What to do if there remain weights or module variables unmatched. Dropout Sets values to zero with probability p class Dropout(seed=None, name=None) seed : the random seed (integer) for initialization, optional .forward(input, p=0.5, shared_axes=(), rescale=True) p : \ufb02oat or scalar tensor. The probability of setting a value to zero shared_axes : tuple of int. Axes to share the dropout mask over. By default, each value can be dropped individually. shared_axes =(0,) uses the same mask across the batch. shared_axes =(2, 3) uses the same mask across the spatial dimensions of 2D feature maps. rescale : bool. If True (the default), scale the input by 1 / (1 - p ) when dropout is enabled, to keep the expected output mean the same. .predict( input, *args, **kwargs) dummy interface, does nothing but returns the input unchanged. Note: Theano uses self_update mechanism to implement pseudo randomness, so to use Dropout class, the followings are recommened: (1) define different instance for each droput layer (2) compiling function with no_default_updates=False GRU Gated Recurrent Unit RNN. The recurrent computation is implemented according to Ref , also the same with Lasagne \\begin{align} r_t &= \\sigma_r(x_t W_{xr} + h_{t - 1} W_{hr} + b_r) \\\\ u_t &= \\sigma_u(x_t W_{xu} + h_{t - 1} W_{hu} + b_u) \\\\ c_t &= \\sigma_c(x_t W_{xc} + r_t \\odot (h_{t - 1} W_{hc}) + b_c) \\\\ h_t &= (1 - u_t) \\odot h_{t - 1} + u_t \\odot c_t \\end{align} \\begin{align} r_t &= \\sigma_r(x_t W_{xr} + h_{t - 1} W_{hr} + b_r) \\\\ u_t &= \\sigma_u(x_t W_{xu} + h_{t - 1} W_{hu} + b_u) \\\\ c_t &= \\sigma_c(x_t W_{xc} + r_t \\odot (h_{t - 1} W_{hc}) + b_c) \\\\ h_t &= (1 - u_t) \\odot h_{t - 1} + u_t \\odot c_t \\end{align} in which \\odot \\odot stands for element-wise multiplication. class GRU(input_dims, hidden_dim, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims ; if list of integers, input dimension = sum( input_dims ), and GRU\u2019s parameter W_in will be initialized unevenly by integers specified in input_dims hidden_dim : dimension of hidden units, also the output dimension grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use tanh as default. learn_ini : whether learn initial state truncate_gradient : if not -1, BPTT will be used, gradient back-propagation will be performed at most truncate_gradient steps .forward(seq_input, h_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False) seq_input : tensor with shape (T, B, D) in which D is the input dimension h_ini : initialization of hidden cell, (B, hidden_dim) seq_mask : mask for seq_input backward : bool. Whether scan in backward direction only_return_final : bool. If True , only return the \ufb01nal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory. return_final_state : If True , the final state of hidden and cell will be returned, both (B, hidden_dim) .predict = .forward LSTM Long Short-Term Memory RNN. The recurrent computation is implemented according to Graves' Generating sequences with recurrent neural networks , also the same with Lasagne: \\begin{align} i_t &= \\sigma_i(x_t W_{xi} + h_{t-1} W_{hi} + w_{ci} \\odot c_{t-1} + b_i) \\\\ f_t &= \\sigma_f(x_t W_{xf} + h_{t-1} W_{hf} + w_{cf} \\odot c_{t-1} + b_f) \\\\ c_t &= f_t \\odot c_{t - 1} + i_t \\odot \\sigma_c(x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\\\ o_t &= \\sigma_o(x_t W_{xo} + h_{t-1} W_{ho} + w_{co} \\odot c_t + b_o) \\\\ h_t &= o_t \\odot \\sigma_h(c_t) \\end{align} \\begin{align} i_t &= \\sigma_i(x_t W_{xi} + h_{t-1} W_{hi} + w_{ci} \\odot c_{t-1} + b_i) \\\\ f_t &= \\sigma_f(x_t W_{xf} + h_{t-1} W_{hf} + w_{cf} \\odot c_{t-1} + b_f) \\\\ c_t &= f_t \\odot c_{t - 1} + i_t \\odot \\sigma_c(x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\\\ o_t &= \\sigma_o(x_t W_{xo} + h_{t-1} W_{ho} + w_{co} \\odot c_t + b_o) \\\\ h_t &= o_t \\odot \\sigma_h(c_t) \\end{align} in which w_{ci}, w_{cf}, w_{co} w_{ci}, w_{cf}, w_{co} are peephole connections. class LSTM( input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims ; if list of integers, input dimension = sum( input_dims ), and LSTM\u2019s parameter W_in will be initialized unevenly by integers specified in input_dims hidden_dim : dimension of hidden units, also the output dimension peephole : bool. Whether add peephole connection. grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use tanh as default. learn_ini : whether learn initial state truncate_gradient : if not -1, BPTT will be used, gradient back-propagation will be performed at most truncate_gradient steps .forward(seq_input, h_ini=None, c_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False) seq_input : tensor with shape (T, B, D) in which D is the input dimension h_ini : initialization of hidden state, (B, hidden_dim) c_ini : initialization of cell state, (B, hidden_dim) seq_mask : mask for seq_input backward : bool. Whether scan in backward direction only_return_final : bool. If True , only return the \ufb01nal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory. return_final_state : If True , the final state of hidden and cell will be returned, both (B, hidden_dim) .predict = .forward LSTM2D 2D LSTM. Input shape now should be (H, W, B, C) , in which C usually comes from CNN's channel dimension. Note for current implementation, LSTM2D usually costs much more memories and runs slower than LSTM (due to graph history memorization which is required by autograd). It's recommended to apply LSTM2D only for small-sized feature maps. As possible alternative, check dandelion.model.Alternate_2D_LSTM \\begin{align} i_t &= \\sigma_i(x_t W_{xi} + h_{left} W_{h_{left}i} + h_{up} W_{h_{up}i} + w_{c_{left}i} \\odot c_{left} + w_{c_{up}i} \\odot c_{up} + b_i) \\\\ f_t &= \\sigma_f(x_t W_{xf} + h_{left} W_{h_{left}f} + h_{up} W_{h_{up}f} + w_{c_{left}f} \\odot c_{left} + w_{c_{up}f} \\odot c_{up} + b_f) \\\\ c_t &= f_t \\odot (c_{left} + c_{up}) * 0.5 + i_t \\odot \\sigma_c(x_t W_{xc} + h_{left} W_{h_{left}c} + h_{up} W_{h_{up}c} + b_c) \\\\ o_t &= \\sigma_o(x_t W_{xo} + h_{left} W_{h_{left}o} + h_{up} W_{h_{up}o} + w_{co} \\odot c_t + b_o) \\\\ h_t &= o_t \\odot \\sigma_h(c_t) \\end{align} \\begin{align} i_t &= \\sigma_i(x_t W_{xi} + h_{left} W_{h_{left}i} + h_{up} W_{h_{up}i} + w_{c_{left}i} \\odot c_{left} + w_{c_{up}i} \\odot c_{up} + b_i) \\\\ f_t &= \\sigma_f(x_t W_{xf} + h_{left} W_{h_{left}f} + h_{up} W_{h_{up}f} + w_{c_{left}f} \\odot c_{left} + w_{c_{up}f} \\odot c_{up} + b_f) \\\\ c_t &= f_t \\odot (c_{left} + c_{up}) * 0.5 + i_t \\odot \\sigma_c(x_t W_{xc} + h_{left} W_{h_{left}c} + h_{up} W_{h_{up}c} + b_c) \\\\ o_t &= \\sigma_o(x_t W_{xo} + h_{left} W_{h_{left}o} + h_{up} W_{h_{up}o} + w_{co} \\odot c_t + b_o) \\\\ h_t &= o_t \\odot \\sigma_h(c_t) \\end{align} Note here t t means the (H, W, ...) shaped input will be scanned with the first 2 dimenions flattened. class LSTM2D(input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims ; if list of integers, input dimension = sum( input_dims ), and LSTM\u2019s parameter W_in will be initialized unevenly by integers specified in input_dims hidden_dim : dimension of hidden units, also the output dimension peephole : bool. Whether add peephole connection. grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use tanh as default. learn_ini : whether learn initial state truncate_gradient : if not -1, BPTT will be used, gradient back-propagation will be performed at most truncate_gradient steps .forward(seq_input, h_ini=None, c_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False) seq_input : tensor with shape (H, W, B, C) in which C is the input dimension h_ini : initialization of hidden state, (B, hidden_dim) c_ini : initialization of cell state, (B, hidden_dim) seq_mask : mask for seq_input backward : bool. Whether scan in backward direction (i.e., right->left, bottom->top) only_return_final : bool. If True , only return the \ufb01nal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory. return_final_state : If True , the final state of hidden and cell will be returned, both (B, hidden_dim) .predict = .forward GRUCell Gated Recurrent Unit RNN Cell class GRUCell(input_dims, hidden_dim, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims ; if list of integers, input dimension = sum( input_dims ), and GRUCell\u2019s parameter W_in will be initialized unevenly by integers specified in input_dims hidden_dim : dimension of hidden units, also the output dimension grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use tanh as default .forward(input, h_pre, mask=None) input : tensor with shape (B, D) in which D is the input dimension h_pre : initialization of hidden cell, (B, hidden_dim) mask : mask for input .predict = .forward LSTMCell Long Short-Term Memory RNN Cell class LSTMCell(input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims ; if list of integers, input dimension = sum( input_dims ), and LSTM\u2019s parameter W_in will be initialized unevenly by integers specified in input_dims hidden_dim : dimension of hidden units, also the output dimension peephole : bool. Whether add peephole connection. grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use tanh as default. .forward(input, h_pre, c_pre, mask=None) input : tensor with shape (B, D) in which D is the input dimension h_pre : initialization of hidden state, (B, hidden_dim) c_pre : initialization of cell state, (B, hidden_dim) mask : mask for input .predict = .forward Conv2D Convolution 2D class Conv2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), pad='valid', dilation=(1,1), num_groups=1, W=init.GlorotUniform(), b=init.Constant(0.), flip_filters=True, convOP=tensor.nnet.conv2d, input_shape=(None,None), untie_bias=False, name=None) input_channels : int. Input shape of Conv2D module is (B, input_channels, H_in, W_in) out_channels : int. Output shape of Conv2D module is (B output_channels, H_out, W_out) kernel_size : int scalar or tuple of int. Convolution kernel size stride : Factor by which to subsample the output pad : same / valid / full or 2-element tuple of int. Control image border padding. dilation : factor by which to subsample (stride) the input. num_groups : Divides the image, kernel and output tensors into num_groups separate groups. Each which carry out convolutions separately W : initialization of filter bank, shape = (out_channels, in_channels, kernel_size[0], kernel_size[1]) b : initialization of convolution bias, shape = (out_channels,) if untie_bias is False; otherwise shape = (out_channels, H_out, W_out) flip_filters : If True , will flip the filter rows and columns before sliding them over the input. This operation is normally referred to as a convolution, and this is the default. If False , the filters are not flipped and the operation is referred to as a cross-correlation. input_shape : optional, (H_in, W_in) untie_bias : If False , the module will have a bias parameter for each channel, which is shared across all positions in this channel. As a result, the b attribute will be a vector (1D). If True , the module will have separate bias parameters for each position in each channel. As a result, the b attribute will be a 3D tensor. ConvTransposed2D Transposed convolution 2D. Also known as fractionally-strided convolution or deconvolution (although it is not an actual deconvolution operation) class ConvTransposed2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), pad='valid', dilation=(1,1), num_groups=1, W=init.GlorotUniform(), b=init.Constant(0.), flip_filters=True, input_shape=(None,None), untie_bias=False, name=None) return : output shape = (B, C, H, W) , in which H = ((H_in - 1) * stride_H) + kernel_H - 2 * pad_H , and the same with W . All the parameters have the same meanings with Conv2D module. In fact, the transposed convolution is equal to upsampling the input then doing conventional convolution. However, for efficiency purpose, here the transposed convolution is implemented via Theano\u2019s AbstractConv2d_gradInputs as what is done in Lasagne. Dense Fully connected network, also known as affine transform. Apply affine transform Wx+b Wx+b to the last dimension of input. The input of Dense can have any dimensions, and note that we do not apply any activation to its output by default class Dense(input_dims, output_dim, W=init.GlorotUniform(), b=init.Constant(0.), name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims; if list of integers, input dimension = sum(input_dims), and Dense\u2019s parameter W will be initialized unevenly by integers specified in input_dims output_dim : output dimension W , b : parameter initialization .forward(input) input : input of any shape .predict = .forward Embedding Word/character embedding module. class Embedding(num_embeddings, embedding_dim, W=init.Normal(), name=None) num_embeddings : the Number of different embeddings embedding_dim : output embedding vector dimension .forward(index_input) index_input : integer tensor .predict = .forward BatchNorm Batch normalization module. The normalization is done as $$ \\begin{align} x' = \\gamma * \\frac{(x-\\mu)}{\\sigma} + \\beta \\end{align} $$ class BatchNorm(input_shape=None, axes='auto', eps=1e-4, alpha=0.1, beta=init.Constant(0), gamma=init.Constant(1), mean=init.Constant(0), inv_std=init.Constant(1), mode='high_mem', name=None) input_shape : Tuple or list of ints or tensor variables. Input shape of BatchNorm module, including batch dimension. axes : auto or tuple of int. The axis or axes to normalize over. If auto (the default), normalize over all axes except for the second: this will normalize over the minibatch dimension for dense layers, and additionally over all spatial dimensions for convolutional layers. eps : Small constant \ud835\udf16 added to the variance before taking the square root and dividing by it, to avoid numerical problems alpha : Coefficient for the exponential moving average of batch-wise means and standard deviations computed during training; the closer to one, the more it will depend on the last batches seen gamma, beta : these two parameters can be set to None to disable the controversial scale and shift. According to Deep Learning Book, Section 8.7.1 , disabling \\gamma \\gamma and \\beta \\beta might reduce the expressive power of the neural network. mode : low_mem or high_mem . Specify which batch normalization implementation that will be used. As no intermediate representations are stored for the back-propagation, low_mem implementation lower the memory usage, however, it is 5-10% slower than high_mem implementation. Note that 5-10% computation time difference compare the batch normalization operation only, time difference between implementation is likely to be less important on the full model fprop/bprop. .forward(input, use_input_mean=True) use_input_mean : default, use mean & std of input batch for normalization; if False , self.mean and self.std will be used for normalization. The reason that input mean is used during training is because at the early training stage, BatchNorm 's self.mean is far from the expected mean value and can be detrimental for network convergence. It's recommended to use input mean for early stage training; after that, you can switch to BatchNorm 's self.mean for training & inference consistency. .predict(input) GroupNorm Group normalization, as described in Group Normalization , only used for CNN output normalization. The normalization is done as $$ \\begin{align} x' = \\gamma * \\frac{(x-\\mu)}{\\sigma} + \\beta, \\ per sample, per group \\end{align} $$ 1) With batch normalization, the normalization is usually done in per-channel way for a CNN output; whereas group normalization operates in per sample and per group way; 2) Group normalization uses sample statistics for both forward and inference stage; 3) Group normalization only applies for 4D input with shape (B, C, H, W) ; 4) Group normalization is recommended when batch size is small; for large batch size, batch normalization is still recommended; 5) Group normalization is equivalent to Layer Normalization when group_num =1; and equivalent to Instance Normalization when group_num = channel_num class GroupNorm(channel_num, group_num=16, eps=1e-5, beta=init.Constant(0), gamma=init.Constant(1), name=None) channel_num : group normalization assumes input of shape (B, C, H, W) , here C = channel_num group_num : group number for CNN channel dimension splitting, channel_num must be divisible by group_num eps : Small constant \ud835\udf16 added to the variance before taking the square root and dividing by it, to avoid numerical problems gamma, beta : these two parameters can be set to None to disable the controversial scale and shift. .forward(input) input : input of shape (B, C, H, W) .predict = .forward Center Estimate class centers by moving averaging. class Center(feature_dim, center_num, center=init.GlorotUniform(), name=None) feature_dim : feature dimension center_num : class center number center : initialization of class centers, should be in shape of (center_num, feature_dim) .forward(features, labels, alpha=0.1) features : batch features, from which the class centers will be estimated labels : features 's corresponding class labels alpha : moving averaging coefficient, the closer to one, the more it will depend on the last batches seen: C_{new} = \\alpha*C_{batch} + (1-\\alpha)*C_{old} C_{new} = \\alpha*C_{batch} + (1-\\alpha)*C_{old} return : centers estimated .predict() return : centers stored ChainCRF Linear chain CRF layer for sequence labeling. class ChainCRF(state_num, transitions=init.GlorotUniform(), p_scale=1.0, l1_regularization=0.001, state_pad=True, transition_matrix_normalization=True, name=None) state_num : number of hidden states. If state_pad is True , then the actual state number inside CRF will be state_num + 2 . transitions : initialization of transition matrix, in shape of (state_num+2, state_num+2) if state_pad is True , else (state_num, state_num) p_scale : probability scale factor. The input of this module will be multiplied by this factor. l1_regularization : L1 regularization coefficient for transition matrix state_pad : whether do state padding. CRF requires two additional dummy states, i.e., <bos> and <eos> (beginning and endding of sequence). The ChainCRF module can pad the state automatically with these two dummy states, or you can incorporate these two states in module input. In the latter case, set state_pad to False . transition_matrix_normalization : whether do row-wise normalization of transition matrix. You may expect that each row of the transition matrix should sum to 1.0, and to do this, set this flag to True . .forward(x, y) Compute CRF loss x : output from previous RNN layer, in shape of (B, T, N) y : tag ground truth, in shape of (B, T), int32 return : loss in shape of (B,) if l1_regularization disabled, else in shape of (1,) .predict(x) CRF Viterbi decoding x : output from previous RNN layer, in shape of (B, T, N) return : decoded sequence Sequential Sequential container for a list of modules, just for convenience. class Sequential(module_list, activation=linear, name=None) module_list : list of network sub-modules, these modules MUST NOT be sub-modules of any other parent module. activation : activation applied to output of each sub-module. Single function or list of functions. If activation is a list, it must be the same length with module_list . .forward(x) Forward pass through the network module sequence. .predict(x) Inference pass through the network module sequence. Example: conv1 = Conv2D(in_channels=1, out_channels=3, stride=(2,2)) bn1 = BatchNorm(input_shape=(None, 3, None, None)) conv2 = Conv2D(in_channels=3, out_channels=5) conv3 = Conv2D(in_channels=5, out_channels=8) model = Sequential([conv1, bn1, conv2, conv3], activation=relu, name='Seq') x = tensor.ftensor4('x') y = model.forward(x) print('compiling fn...') fn = theano.function([x], y, no_default_updates=False) print('run fn...') input = np.random.rand(4, 1, 32, 33).astype(np.float32) output = fn(input) print(output)","title":"dandelion.module"},{"location":"dandelion_module/#module","text":"Root class of all network modules, you'd always subclass this for a new module class Module(name=None, work_mode='inference') name : module name, optional. If you don't specify the module name, it will be auto-named if this module is a sub-module of another module. work_mode : working mode, optional. Only used for the unified calling interface, check \"Tutorial I\" for detailed explanation. .params = [] .self_updating_variables = [] .sub_modules = OrderedDict() .name = name .work_mode = work_mode params : contains all the parameters which should be updated by optimizer (submodule excluded) self_updating_variables : contains all the parameters which are updated by user specified expression (submodule excluded) sub_modules : contains all the sub-modules .register_param(x, shape=None, name=None) .register_self_updating_variable(x, shape=None, name=None) Register and possibly initialize a parameter tensor. Parameters to be updated by optimizer should be registered with register_param() meanwhile parameters self-updated should be registerd with register_self_updating_variable() x : Theano shared variable, expression, numpy array or callable. Initial value, expression or initializer for this parameter. shape : tuple of int, optional. A tuple of integers representing the desired shape of the parameter tensor. name : str, optional. It's recommended to let the Dandelion framework name the variable automatically. .collect_params(include=None, exclude=None, include_self=True) Collect parameters to be updated by optimizer. include : sub-module keys, means which sub-module to include exclude : sub-module keys, means which sub-module to exclude include_self : whether include self.params return : list of parameters, in the same order of sub-modules .collect_self_updates(include=None, exclude=None, include_self=True) Collect all update from self_updating_variables. include : sub-module keys, means which sub-module to include exclude : sub-module keys, means which sub-module to exclude include_self : whether include self.self_updating_variables return : update dict, in the same order of sub-modules .get_weights() Collect all module weights (including submodules) return : list of tuples with format [variable.value, variable.name] .set_weights(module_weights, check_name='ignore') Set module weights by default order (same order with .get_weights() ) module_weights : same with the return of .get_weights() check_name : ignore | warn | raise . What to do if a weight's name does not match its corresponding variable's name. .set_weights_by_name(module_weights, unmatched='raise') Set module weights by matching name. module_weights : same with the return of .get_weights() unmatched : ignore | warn | raise . What to do if there remain weights or module variables unmatched.","title":"Module"},{"location":"dandelion_module/#dropout","text":"Sets values to zero with probability p class Dropout(seed=None, name=None) seed : the random seed (integer) for initialization, optional .forward(input, p=0.5, shared_axes=(), rescale=True) p : \ufb02oat or scalar tensor. The probability of setting a value to zero shared_axes : tuple of int. Axes to share the dropout mask over. By default, each value can be dropped individually. shared_axes =(0,) uses the same mask across the batch. shared_axes =(2, 3) uses the same mask across the spatial dimensions of 2D feature maps. rescale : bool. If True (the default), scale the input by 1 / (1 - p ) when dropout is enabled, to keep the expected output mean the same. .predict( input, *args, **kwargs) dummy interface, does nothing but returns the input unchanged. Note: Theano uses self_update mechanism to implement pseudo randomness, so to use Dropout class, the followings are recommened: (1) define different instance for each droput layer (2) compiling function with no_default_updates=False","title":"Dropout"},{"location":"dandelion_module/#gru","text":"Gated Recurrent Unit RNN. The recurrent computation is implemented according to Ref , also the same with Lasagne \\begin{align} r_t &= \\sigma_r(x_t W_{xr} + h_{t - 1} W_{hr} + b_r) \\\\ u_t &= \\sigma_u(x_t W_{xu} + h_{t - 1} W_{hu} + b_u) \\\\ c_t &= \\sigma_c(x_t W_{xc} + r_t \\odot (h_{t - 1} W_{hc}) + b_c) \\\\ h_t &= (1 - u_t) \\odot h_{t - 1} + u_t \\odot c_t \\end{align} \\begin{align} r_t &= \\sigma_r(x_t W_{xr} + h_{t - 1} W_{hr} + b_r) \\\\ u_t &= \\sigma_u(x_t W_{xu} + h_{t - 1} W_{hu} + b_u) \\\\ c_t &= \\sigma_c(x_t W_{xc} + r_t \\odot (h_{t - 1} W_{hc}) + b_c) \\\\ h_t &= (1 - u_t) \\odot h_{t - 1} + u_t \\odot c_t \\end{align} in which \\odot \\odot stands for element-wise multiplication. class GRU(input_dims, hidden_dim, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims ; if list of integers, input dimension = sum( input_dims ), and GRU\u2019s parameter W_in will be initialized unevenly by integers specified in input_dims hidden_dim : dimension of hidden units, also the output dimension grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use tanh as default. learn_ini : whether learn initial state truncate_gradient : if not -1, BPTT will be used, gradient back-propagation will be performed at most truncate_gradient steps .forward(seq_input, h_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False) seq_input : tensor with shape (T, B, D) in which D is the input dimension h_ini : initialization of hidden cell, (B, hidden_dim) seq_mask : mask for seq_input backward : bool. Whether scan in backward direction only_return_final : bool. If True , only return the \ufb01nal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory. return_final_state : If True , the final state of hidden and cell will be returned, both (B, hidden_dim) .predict = .forward","title":"GRU"},{"location":"dandelion_module/#lstm","text":"Long Short-Term Memory RNN. The recurrent computation is implemented according to Graves' Generating sequences with recurrent neural networks , also the same with Lasagne: \\begin{align} i_t &= \\sigma_i(x_t W_{xi} + h_{t-1} W_{hi} + w_{ci} \\odot c_{t-1} + b_i) \\\\ f_t &= \\sigma_f(x_t W_{xf} + h_{t-1} W_{hf} + w_{cf} \\odot c_{t-1} + b_f) \\\\ c_t &= f_t \\odot c_{t - 1} + i_t \\odot \\sigma_c(x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\\\ o_t &= \\sigma_o(x_t W_{xo} + h_{t-1} W_{ho} + w_{co} \\odot c_t + b_o) \\\\ h_t &= o_t \\odot \\sigma_h(c_t) \\end{align} \\begin{align} i_t &= \\sigma_i(x_t W_{xi} + h_{t-1} W_{hi} + w_{ci} \\odot c_{t-1} + b_i) \\\\ f_t &= \\sigma_f(x_t W_{xf} + h_{t-1} W_{hf} + w_{cf} \\odot c_{t-1} + b_f) \\\\ c_t &= f_t \\odot c_{t - 1} + i_t \\odot \\sigma_c(x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\\\ o_t &= \\sigma_o(x_t W_{xo} + h_{t-1} W_{ho} + w_{co} \\odot c_t + b_o) \\\\ h_t &= o_t \\odot \\sigma_h(c_t) \\end{align} in which w_{ci}, w_{cf}, w_{co} w_{ci}, w_{cf}, w_{co} are peephole connections. class LSTM( input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims ; if list of integers, input dimension = sum( input_dims ), and LSTM\u2019s parameter W_in will be initialized unevenly by integers specified in input_dims hidden_dim : dimension of hidden units, also the output dimension peephole : bool. Whether add peephole connection. grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use tanh as default. learn_ini : whether learn initial state truncate_gradient : if not -1, BPTT will be used, gradient back-propagation will be performed at most truncate_gradient steps .forward(seq_input, h_ini=None, c_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False) seq_input : tensor with shape (T, B, D) in which D is the input dimension h_ini : initialization of hidden state, (B, hidden_dim) c_ini : initialization of cell state, (B, hidden_dim) seq_mask : mask for seq_input backward : bool. Whether scan in backward direction only_return_final : bool. If True , only return the \ufb01nal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory. return_final_state : If True , the final state of hidden and cell will be returned, both (B, hidden_dim) .predict = .forward","title":"LSTM"},{"location":"dandelion_module/#lstm2d","text":"2D LSTM. Input shape now should be (H, W, B, C) , in which C usually comes from CNN's channel dimension. Note for current implementation, LSTM2D usually costs much more memories and runs slower than LSTM (due to graph history memorization which is required by autograd). It's recommended to apply LSTM2D only for small-sized feature maps. As possible alternative, check dandelion.model.Alternate_2D_LSTM \\begin{align} i_t &= \\sigma_i(x_t W_{xi} + h_{left} W_{h_{left}i} + h_{up} W_{h_{up}i} + w_{c_{left}i} \\odot c_{left} + w_{c_{up}i} \\odot c_{up} + b_i) \\\\ f_t &= \\sigma_f(x_t W_{xf} + h_{left} W_{h_{left}f} + h_{up} W_{h_{up}f} + w_{c_{left}f} \\odot c_{left} + w_{c_{up}f} \\odot c_{up} + b_f) \\\\ c_t &= f_t \\odot (c_{left} + c_{up}) * 0.5 + i_t \\odot \\sigma_c(x_t W_{xc} + h_{left} W_{h_{left}c} + h_{up} W_{h_{up}c} + b_c) \\\\ o_t &= \\sigma_o(x_t W_{xo} + h_{left} W_{h_{left}o} + h_{up} W_{h_{up}o} + w_{co} \\odot c_t + b_o) \\\\ h_t &= o_t \\odot \\sigma_h(c_t) \\end{align} \\begin{align} i_t &= \\sigma_i(x_t W_{xi} + h_{left} W_{h_{left}i} + h_{up} W_{h_{up}i} + w_{c_{left}i} \\odot c_{left} + w_{c_{up}i} \\odot c_{up} + b_i) \\\\ f_t &= \\sigma_f(x_t W_{xf} + h_{left} W_{h_{left}f} + h_{up} W_{h_{up}f} + w_{c_{left}f} \\odot c_{left} + w_{c_{up}f} \\odot c_{up} + b_f) \\\\ c_t &= f_t \\odot (c_{left} + c_{up}) * 0.5 + i_t \\odot \\sigma_c(x_t W_{xc} + h_{left} W_{h_{left}c} + h_{up} W_{h_{up}c} + b_c) \\\\ o_t &= \\sigma_o(x_t W_{xo} + h_{left} W_{h_{left}o} + h_{up} W_{h_{up}o} + w_{co} \\odot c_t + b_o) \\\\ h_t &= o_t \\odot \\sigma_h(c_t) \\end{align} Note here t t means the (H, W, ...) shaped input will be scanned with the first 2 dimenions flattened. class LSTM2D(input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims ; if list of integers, input dimension = sum( input_dims ), and LSTM\u2019s parameter W_in will be initialized unevenly by integers specified in input_dims hidden_dim : dimension of hidden units, also the output dimension peephole : bool. Whether add peephole connection. grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use tanh as default. learn_ini : whether learn initial state truncate_gradient : if not -1, BPTT will be used, gradient back-propagation will be performed at most truncate_gradient steps .forward(seq_input, h_ini=None, c_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False) seq_input : tensor with shape (H, W, B, C) in which C is the input dimension h_ini : initialization of hidden state, (B, hidden_dim) c_ini : initialization of cell state, (B, hidden_dim) seq_mask : mask for seq_input backward : bool. Whether scan in backward direction (i.e., right->left, bottom->top) only_return_final : bool. If True , only return the \ufb01nal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory. return_final_state : If True , the final state of hidden and cell will be returned, both (B, hidden_dim) .predict = .forward","title":"LSTM2D"},{"location":"dandelion_module/#grucell","text":"Gated Recurrent Unit RNN Cell class GRUCell(input_dims, hidden_dim, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims ; if list of integers, input dimension = sum( input_dims ), and GRUCell\u2019s parameter W_in will be initialized unevenly by integers specified in input_dims hidden_dim : dimension of hidden units, also the output dimension grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use tanh as default .forward(input, h_pre, mask=None) input : tensor with shape (B, D) in which D is the input dimension h_pre : initialization of hidden cell, (B, hidden_dim) mask : mask for input .predict = .forward","title":"GRUCell"},{"location":"dandelion_module/#lstmcell","text":"Long Short-Term Memory RNN Cell class LSTMCell(input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, hidden_activation=tanh, name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims ; if list of integers, input dimension = sum( input_dims ), and LSTM\u2019s parameter W_in will be initialized unevenly by integers specified in input_dims hidden_dim : dimension of hidden units, also the output dimension peephole : bool. Whether add peephole connection. grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use tanh as default. .forward(input, h_pre, c_pre, mask=None) input : tensor with shape (B, D) in which D is the input dimension h_pre : initialization of hidden state, (B, hidden_dim) c_pre : initialization of cell state, (B, hidden_dim) mask : mask for input .predict = .forward","title":"LSTMCell"},{"location":"dandelion_module/#conv2d","text":"Convolution 2D class Conv2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), pad='valid', dilation=(1,1), num_groups=1, W=init.GlorotUniform(), b=init.Constant(0.), flip_filters=True, convOP=tensor.nnet.conv2d, input_shape=(None,None), untie_bias=False, name=None) input_channels : int. Input shape of Conv2D module is (B, input_channels, H_in, W_in) out_channels : int. Output shape of Conv2D module is (B output_channels, H_out, W_out) kernel_size : int scalar or tuple of int. Convolution kernel size stride : Factor by which to subsample the output pad : same / valid / full or 2-element tuple of int. Control image border padding. dilation : factor by which to subsample (stride) the input. num_groups : Divides the image, kernel and output tensors into num_groups separate groups. Each which carry out convolutions separately W : initialization of filter bank, shape = (out_channels, in_channels, kernel_size[0], kernel_size[1]) b : initialization of convolution bias, shape = (out_channels,) if untie_bias is False; otherwise shape = (out_channels, H_out, W_out) flip_filters : If True , will flip the filter rows and columns before sliding them over the input. This operation is normally referred to as a convolution, and this is the default. If False , the filters are not flipped and the operation is referred to as a cross-correlation. input_shape : optional, (H_in, W_in) untie_bias : If False , the module will have a bias parameter for each channel, which is shared across all positions in this channel. As a result, the b attribute will be a vector (1D). If True , the module will have separate bias parameters for each position in each channel. As a result, the b attribute will be a 3D tensor.","title":"Conv2D"},{"location":"dandelion_module/#convtransposed2d","text":"Transposed convolution 2D. Also known as fractionally-strided convolution or deconvolution (although it is not an actual deconvolution operation) class ConvTransposed2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), pad='valid', dilation=(1,1), num_groups=1, W=init.GlorotUniform(), b=init.Constant(0.), flip_filters=True, input_shape=(None,None), untie_bias=False, name=None) return : output shape = (B, C, H, W) , in which H = ((H_in - 1) * stride_H) + kernel_H - 2 * pad_H , and the same with W . All the parameters have the same meanings with Conv2D module. In fact, the transposed convolution is equal to upsampling the input then doing conventional convolution. However, for efficiency purpose, here the transposed convolution is implemented via Theano\u2019s AbstractConv2d_gradInputs as what is done in Lasagne.","title":"ConvTransposed2D"},{"location":"dandelion_module/#dense","text":"Fully connected network, also known as affine transform. Apply affine transform Wx+b Wx+b to the last dimension of input. The input of Dense can have any dimensions, and note that we do not apply any activation to its output by default class Dense(input_dims, output_dim, W=init.GlorotUniform(), b=init.Constant(0.), name=None) input_dims : integer or list of integers. If scalar, input dimension = input_dims; if list of integers, input dimension = sum(input_dims), and Dense\u2019s parameter W will be initialized unevenly by integers specified in input_dims output_dim : output dimension W , b : parameter initialization .forward(input) input : input of any shape .predict = .forward","title":"Dense"},{"location":"dandelion_module/#embedding","text":"Word/character embedding module. class Embedding(num_embeddings, embedding_dim, W=init.Normal(), name=None) num_embeddings : the Number of different embeddings embedding_dim : output embedding vector dimension .forward(index_input) index_input : integer tensor .predict = .forward","title":"Embedding"},{"location":"dandelion_module/#batchnorm","text":"Batch normalization module. The normalization is done as $$ \\begin{align} x' = \\gamma * \\frac{(x-\\mu)}{\\sigma} + \\beta \\end{align} $$ class BatchNorm(input_shape=None, axes='auto', eps=1e-4, alpha=0.1, beta=init.Constant(0), gamma=init.Constant(1), mean=init.Constant(0), inv_std=init.Constant(1), mode='high_mem', name=None) input_shape : Tuple or list of ints or tensor variables. Input shape of BatchNorm module, including batch dimension. axes : auto or tuple of int. The axis or axes to normalize over. If auto (the default), normalize over all axes except for the second: this will normalize over the minibatch dimension for dense layers, and additionally over all spatial dimensions for convolutional layers. eps : Small constant \ud835\udf16 added to the variance before taking the square root and dividing by it, to avoid numerical problems alpha : Coefficient for the exponential moving average of batch-wise means and standard deviations computed during training; the closer to one, the more it will depend on the last batches seen gamma, beta : these two parameters can be set to None to disable the controversial scale and shift. According to Deep Learning Book, Section 8.7.1 , disabling \\gamma \\gamma and \\beta \\beta might reduce the expressive power of the neural network. mode : low_mem or high_mem . Specify which batch normalization implementation that will be used. As no intermediate representations are stored for the back-propagation, low_mem implementation lower the memory usage, however, it is 5-10% slower than high_mem implementation. Note that 5-10% computation time difference compare the batch normalization operation only, time difference between implementation is likely to be less important on the full model fprop/bprop. .forward(input, use_input_mean=True) use_input_mean : default, use mean & std of input batch for normalization; if False , self.mean and self.std will be used for normalization. The reason that input mean is used during training is because at the early training stage, BatchNorm 's self.mean is far from the expected mean value and can be detrimental for network convergence. It's recommended to use input mean for early stage training; after that, you can switch to BatchNorm 's self.mean for training & inference consistency. .predict(input)","title":"BatchNorm"},{"location":"dandelion_module/#groupnorm","text":"Group normalization, as described in Group Normalization , only used for CNN output normalization. The normalization is done as $$ \\begin{align} x' = \\gamma * \\frac{(x-\\mu)}{\\sigma} + \\beta, \\ per sample, per group \\end{align} $$ 1) With batch normalization, the normalization is usually done in per-channel way for a CNN output; whereas group normalization operates in per sample and per group way; 2) Group normalization uses sample statistics for both forward and inference stage; 3) Group normalization only applies for 4D input with shape (B, C, H, W) ; 4) Group normalization is recommended when batch size is small; for large batch size, batch normalization is still recommended; 5) Group normalization is equivalent to Layer Normalization when group_num =1; and equivalent to Instance Normalization when group_num = channel_num class GroupNorm(channel_num, group_num=16, eps=1e-5, beta=init.Constant(0), gamma=init.Constant(1), name=None) channel_num : group normalization assumes input of shape (B, C, H, W) , here C = channel_num group_num : group number for CNN channel dimension splitting, channel_num must be divisible by group_num eps : Small constant \ud835\udf16 added to the variance before taking the square root and dividing by it, to avoid numerical problems gamma, beta : these two parameters can be set to None to disable the controversial scale and shift. .forward(input) input : input of shape (B, C, H, W) .predict = .forward","title":"GroupNorm"},{"location":"dandelion_module/#center","text":"Estimate class centers by moving averaging. class Center(feature_dim, center_num, center=init.GlorotUniform(), name=None) feature_dim : feature dimension center_num : class center number center : initialization of class centers, should be in shape of (center_num, feature_dim) .forward(features, labels, alpha=0.1) features : batch features, from which the class centers will be estimated labels : features 's corresponding class labels alpha : moving averaging coefficient, the closer to one, the more it will depend on the last batches seen: C_{new} = \\alpha*C_{batch} + (1-\\alpha)*C_{old} C_{new} = \\alpha*C_{batch} + (1-\\alpha)*C_{old} return : centers estimated .predict() return : centers stored","title":"Center"},{"location":"dandelion_module/#chaincrf","text":"Linear chain CRF layer for sequence labeling. class ChainCRF(state_num, transitions=init.GlorotUniform(), p_scale=1.0, l1_regularization=0.001, state_pad=True, transition_matrix_normalization=True, name=None) state_num : number of hidden states. If state_pad is True , then the actual state number inside CRF will be state_num + 2 . transitions : initialization of transition matrix, in shape of (state_num+2, state_num+2) if state_pad is True , else (state_num, state_num) p_scale : probability scale factor. The input of this module will be multiplied by this factor. l1_regularization : L1 regularization coefficient for transition matrix state_pad : whether do state padding. CRF requires two additional dummy states, i.e., <bos> and <eos> (beginning and endding of sequence). The ChainCRF module can pad the state automatically with these two dummy states, or you can incorporate these two states in module input. In the latter case, set state_pad to False . transition_matrix_normalization : whether do row-wise normalization of transition matrix. You may expect that each row of the transition matrix should sum to 1.0, and to do this, set this flag to True . .forward(x, y) Compute CRF loss x : output from previous RNN layer, in shape of (B, T, N) y : tag ground truth, in shape of (B, T), int32 return : loss in shape of (B,) if l1_regularization disabled, else in shape of (1,) .predict(x) CRF Viterbi decoding x : output from previous RNN layer, in shape of (B, T, N) return : decoded sequence","title":"ChainCRF"},{"location":"dandelion_module/#sequential","text":"Sequential container for a list of modules, just for convenience. class Sequential(module_list, activation=linear, name=None) module_list : list of network sub-modules, these modules MUST NOT be sub-modules of any other parent module. activation : activation applied to output of each sub-module. Single function or list of functions. If activation is a list, it must be the same length with module_list . .forward(x) Forward pass through the network module sequence. .predict(x) Inference pass through the network module sequence. Example: conv1 = Conv2D(in_channels=1, out_channels=3, stride=(2,2)) bn1 = BatchNorm(input_shape=(None, 3, None, None)) conv2 = Conv2D(in_channels=3, out_channels=5) conv3 = Conv2D(in_channels=5, out_channels=8) model = Sequential([conv1, bn1, conv2, conv3], activation=relu, name='Seq') x = tensor.ftensor4('x') y = model.forward(x) print('compiling fn...') fn = theano.function([x], y, no_default_updates=False) print('run fn...') input = np.random.rand(4, 1, 32, 33).astype(np.float32) output = fn(input) print(output)","title":"Sequential"},{"location":"dandelion_objective/","text":"ctc_cost_logscale CTC cost calculated in log scale. This CTC objective is written purely in Theano, so it runs on both Windows and Linux. Theano itself also has a wrapper for Baidu's warp-ctc library, which requires separate install and only runs on Linux. ctc_cost_logscale(seq, sm, seq_mask=None, sm_mask=None, blank_symbol=None, align='pre') seq : query sequence, shape of (L, B) , float32 -typed sm : score matrix, shape of (T, C+1, B) , float32 -typed seq_mask : mask for query sequence, shape of (L, B) , float32 -typed sm_mask : mask for score matrix, shape of (T, B) , float32 -typed blank_symbol : scalar, = C by default align : string, {'pre'/'post'}, indicating how input samples are aligned in one batch return : negative log likelihood averaged over a batch ctc_best_path_decode Decode the network output scorematrix by best-path-decoding scheme. ctc_best_path_decode(Y, Y_mask=None, blank_symbol=None) Y : output of a network, with shape (B, T, C+1) Y_mask : mask of Y, with shape (B, T) return : result sequence of shape (T, B ), and result sequence mask of shape (T, B) ctc_CER Calculate the character error rate (CER) given ground truth targetseq and CTC decoding output resultseq ctc_CER(resultseq, targetseq, resultseq_mask=None, targetseq_mask=None) resultseq : CTC decoding output, with shape (T1, B) targetseq : sequence ground truth, with shape (T2, B) return : tuple of (CER, TE, TG) , in which TE is the batch-wise total edit distance, TG is the batch-wise total ground truth sequence length, and CER equals to TE/TG categorical_crossentropy Computes the categorical cross-entropy between predictions and targets categorical_crossentropy(predictions, targets, eps=1e-7, m=None, class_weight=None) predictions : Theano 2D tensor, predictions in (0, 1), such as softmax output of a neural network, with data points in rows and class probabilities in columns. targets : Theano 2D tensor or 1D tensor, either targets in [0, 1] (float32 type) matching the layout of predictions , or a vector of int giving the correct class index per data point. In the case of an integer vector argument, each element represents the position of the '1' in a one-hot encoding. eps : epsilon added to predictions to prevent numerical unstability when using with softmax activation m : possible max value of targets 's element, required when targets is 1D tensor and class_weight is not None. class_weight : tensor vector with shape (Nclass,), used for class weighting, optional. return : Theano 1D tensor, an expression for the item-wise categorical cross-entropy. categorical_crossentropy_log Computes the categorical cross-entropy between predictions and targets, in log-domain. categorical_crossentropy_log(log_predictions, targets, m=None, class_weight=None) log_predictions : Theano 2D tensor, predictions in log of (0, 1), such as log_softmax output of a neural network, with data points in rows and class probabilities in columns. targets : Theano 2D tensor or 1D tensor, either targets in [0, 1] (float32 type) matching the layout of predictions , or a vector of int giving the correct class index per data point. In the case of an integer vector argument, each element represents the position of the '1' in a one-hot encoding. m : possible max value of targets 's element, only used when targets is 1D vector. When targets is integer vector, the implementation of categorical_crossentropy_log is different from categorical_crossentropy : the latter relies on theano.tensor.nnet.categorical_crossentropy whereas the former uses a simpler way, we transform the integer vector targets into one-hot encoded matrix. That's why we need the m argument here. The possible limitation is that our implementation does not allow m changing on-the-fly. class_weight : tensor vector with shape (Nclass,), used for class weighting, optional. return : Theano 1D tensor, an expression for the item-wise categorical cross-entropy in log-domain You're recommended to refer to Lasagne.objectives document for the following objectives: binary_crossentropy squared_error binary_hinge_loss multiclass_hinge_loss binary_accuracy categorical_accuracy","title":"dandelion.objective"},{"location":"dandelion_objective/#ctc_cost_logscale","text":"CTC cost calculated in log scale. This CTC objective is written purely in Theano, so it runs on both Windows and Linux. Theano itself also has a wrapper for Baidu's warp-ctc library, which requires separate install and only runs on Linux. ctc_cost_logscale(seq, sm, seq_mask=None, sm_mask=None, blank_symbol=None, align='pre') seq : query sequence, shape of (L, B) , float32 -typed sm : score matrix, shape of (T, C+1, B) , float32 -typed seq_mask : mask for query sequence, shape of (L, B) , float32 -typed sm_mask : mask for score matrix, shape of (T, B) , float32 -typed blank_symbol : scalar, = C by default align : string, {'pre'/'post'}, indicating how input samples are aligned in one batch return : negative log likelihood averaged over a batch","title":"ctc_cost_logscale"},{"location":"dandelion_objective/#ctc_best_path_decode","text":"Decode the network output scorematrix by best-path-decoding scheme. ctc_best_path_decode(Y, Y_mask=None, blank_symbol=None) Y : output of a network, with shape (B, T, C+1) Y_mask : mask of Y, with shape (B, T) return : result sequence of shape (T, B ), and result sequence mask of shape (T, B)","title":"ctc_best_path_decode"},{"location":"dandelion_objective/#ctc_cer","text":"Calculate the character error rate (CER) given ground truth targetseq and CTC decoding output resultseq ctc_CER(resultseq, targetseq, resultseq_mask=None, targetseq_mask=None) resultseq : CTC decoding output, with shape (T1, B) targetseq : sequence ground truth, with shape (T2, B) return : tuple of (CER, TE, TG) , in which TE is the batch-wise total edit distance, TG is the batch-wise total ground truth sequence length, and CER equals to TE/TG","title":"ctc_CER"},{"location":"dandelion_objective/#categorical_crossentropy","text":"Computes the categorical cross-entropy between predictions and targets categorical_crossentropy(predictions, targets, eps=1e-7, m=None, class_weight=None) predictions : Theano 2D tensor, predictions in (0, 1), such as softmax output of a neural network, with data points in rows and class probabilities in columns. targets : Theano 2D tensor or 1D tensor, either targets in [0, 1] (float32 type) matching the layout of predictions , or a vector of int giving the correct class index per data point. In the case of an integer vector argument, each element represents the position of the '1' in a one-hot encoding. eps : epsilon added to predictions to prevent numerical unstability when using with softmax activation m : possible max value of targets 's element, required when targets is 1D tensor and class_weight is not None. class_weight : tensor vector with shape (Nclass,), used for class weighting, optional. return : Theano 1D tensor, an expression for the item-wise categorical cross-entropy.","title":"categorical_crossentropy"},{"location":"dandelion_objective/#categorical_crossentropy_log","text":"Computes the categorical cross-entropy between predictions and targets, in log-domain. categorical_crossentropy_log(log_predictions, targets, m=None, class_weight=None) log_predictions : Theano 2D tensor, predictions in log of (0, 1), such as log_softmax output of a neural network, with data points in rows and class probabilities in columns. targets : Theano 2D tensor or 1D tensor, either targets in [0, 1] (float32 type) matching the layout of predictions , or a vector of int giving the correct class index per data point. In the case of an integer vector argument, each element represents the position of the '1' in a one-hot encoding. m : possible max value of targets 's element, only used when targets is 1D vector. When targets is integer vector, the implementation of categorical_crossentropy_log is different from categorical_crossentropy : the latter relies on theano.tensor.nnet.categorical_crossentropy whereas the former uses a simpler way, we transform the integer vector targets into one-hot encoded matrix. That's why we need the m argument here. The possible limitation is that our implementation does not allow m changing on-the-fly. class_weight : tensor vector with shape (Nclass,), used for class weighting, optional. return : Theano 1D tensor, an expression for the item-wise categorical cross-entropy in log-domain You're recommended to refer to Lasagne.objectives document for the following objectives: binary_crossentropy squared_error binary_hinge_loss multiclass_hinge_loss binary_accuracy categorical_accuracy","title":"categorical_crossentropy_log"},{"location":"dandelion_update/","text":"Dandelion's update module is mostly inherited from Lasagne , you're recommended to refer to Lasagne.updates document for the following optimizers & helper functions: apply_momentum momentum apply_nesterov_momentum nesterov_momentum adagrad rmsprop adamax norm_constraint total_norm_constrain sgd Stochastic gradient descent optimizer. sgd(loss_or_grads, params, learning_rate=1e-4, clear_nan=False) loss_or_grads : a scalar loss expression, or a list of gradient expressions params : list of shared variables to generate update expressions for learning_rate : float or symbolic scalar, learning rate controlling the size of update steps clear_nan : boolean flag, if True , nan in gradients will be replaced with 0 adam Adam optimizer implemented as described in \"Kingma, Diederik, and Jimmy Ba (2014): Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980.\" adam(loss_or_grads, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, clear_nan=False) loss_or_grads : a scalar loss expression, or a list of gradient expressions params : list of shared variables to generate update expressions for learning_rate : float or symbolic scalar, learning rate controlling the size of update steps clear_nan : boolean flag, if True , nan in gradients will be replaced with 0 beta1 : float or symbolic scalar, exponential decay rate for the first moment estimates beta2 : float or symbolic scalar, exponential decay rate for the second moment estimates epsilon : float or symbolic scalar, constant for numerical stability adadelta Adadelta optimizer implemented as described in \"Zeiler, M. D. (2012): ADADELTA: An Adaptive Learning Rate Method. arXiv Preprint arXiv:1212.5701.\" adadelta(loss_or_grads, params, learning_rate=1.0, rho=0.95, epsilon=1e-6, clear_nan=False) loss_or_grads : a scalar loss expression, or a list of gradient expressions params : list of shared variables to generate update expressions for learning_rate : float or symbolic scalar, learning rate controlling the size of update steps clear_nan : boolean flag, if True , nan in gradients will be replaced with 0 rho : float or symbolic scalar, squared gradient moving average decay factor epsilon : float or symbolic scalar, constant for numerical stability rho should be between 0 and 1. A value of rho close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast. rho = 0.95 and epsilon =1e-6 are suggested in the paper and reported to work for multiple datasets (MNIST, speech). In the paper, no learning rate is considered (so learning_rate =1.0). Probably best to keep it at this value. epsilon is important for the very first update (so the numerator does not become 0).","title":"dandelion.update"},{"location":"dandelion_update/#sgd","text":"Stochastic gradient descent optimizer. sgd(loss_or_grads, params, learning_rate=1e-4, clear_nan=False) loss_or_grads : a scalar loss expression, or a list of gradient expressions params : list of shared variables to generate update expressions for learning_rate : float or symbolic scalar, learning rate controlling the size of update steps clear_nan : boolean flag, if True , nan in gradients will be replaced with 0","title":"sgd"},{"location":"dandelion_update/#adam","text":"Adam optimizer implemented as described in \"Kingma, Diederik, and Jimmy Ba (2014): Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980.\" adam(loss_or_grads, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, clear_nan=False) loss_or_grads : a scalar loss expression, or a list of gradient expressions params : list of shared variables to generate update expressions for learning_rate : float or symbolic scalar, learning rate controlling the size of update steps clear_nan : boolean flag, if True , nan in gradients will be replaced with 0 beta1 : float or symbolic scalar, exponential decay rate for the first moment estimates beta2 : float or symbolic scalar, exponential decay rate for the second moment estimates epsilon : float or symbolic scalar, constant for numerical stability","title":"adam"},{"location":"dandelion_update/#adadelta","text":"Adadelta optimizer implemented as described in \"Zeiler, M. D. (2012): ADADELTA: An Adaptive Learning Rate Method. arXiv Preprint arXiv:1212.5701.\" adadelta(loss_or_grads, params, learning_rate=1.0, rho=0.95, epsilon=1e-6, clear_nan=False) loss_or_grads : a scalar loss expression, or a list of gradient expressions params : list of shared variables to generate update expressions for learning_rate : float or symbolic scalar, learning rate controlling the size of update steps clear_nan : boolean flag, if True , nan in gradients will be replaced with 0 rho : float or symbolic scalar, squared gradient moving average decay factor epsilon : float or symbolic scalar, constant for numerical stability rho should be between 0 and 1. A value of rho close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast. rho = 0.95 and epsilon =1e-6 are suggested in the paper and reported to work for multiple datasets (MNIST, speech). In the paper, no learning rate is considered (so learning_rate =1.0). Probably best to keep it at this value. epsilon is important for the very first update (so the numerator does not become 0).","title":"adadelta"},{"location":"dandelion_util/","text":"gpickle Pickle with gzip enabled. .dump(data, filename, compresslevel=9) data : data to be dumped to file filename : file path compresslevel : gzip compression level, default = 9. .load(filename) filename : file to be loaded theano_safe_run Help catch theano memory exceptions during running theano function theano_safe_run(fn, input_list) fn : theano function to run input_list : list of input arguments return : errcode and funtion excution result theano_safe_run() catches the following 4 memory exceptions (range from theano 0.x to 1.x): MemoryError. errcode=1 CudaNdarray_ZEROS: allocation failed. errcode=2 gpudata_alloc: cuMemAlloc: CUDA_ERROR_OUT_OF_MEMORY: out of memory. errcode=3 cuMemAlloc: CUDA_ERROR_OUT_OF_MEMORY: out of memory. errcode=4","title":"dandelion.util"},{"location":"dandelion_util/#gpickle","text":"Pickle with gzip enabled. .dump(data, filename, compresslevel=9) data : data to be dumped to file filename : file path compresslevel : gzip compression level, default = 9. .load(filename) filename : file to be loaded","title":"gpickle"},{"location":"dandelion_util/#theano_safe_run","text":"Help catch theano memory exceptions during running theano function theano_safe_run(fn, input_list) fn : theano function to run input_list : list of input arguments return : errcode and funtion excution result theano_safe_run() catches the following 4 memory exceptions (range from theano 0.x to 1.x): MemoryError. errcode=1 CudaNdarray_ZEROS: allocation failed. errcode=2 gpudata_alloc: cuMemAlloc: CUDA_ERROR_OUT_OF_MEMORY: out of memory. errcode=3 cuMemAlloc: CUDA_ERROR_OUT_OF_MEMORY: out of memory. errcode=4","title":"theano_safe_run"},{"location":"history/","text":"History version 0.17.19 [1-23-2019] NEW : add clear_nan argument for sgd , adam and adadelta optimizers. MODIFIED : add default value 1e-4 for sgd 's learning_rate arg. version 0.17.17 [11-22-2018] NEW : add GroupNorm module for group normalization implementation; MODIFIED : expose dim_broadcast arg for Module.register_param() method; MODIFIED : replace spec = tensor.patternbroadcast(spec, dim_broadcast) with spec = theano.shared(spec, broadcastable=dim_broadcast) for util.create_param() , due to the former would change tensor's type. version 0.17.16 [11-19-2018] FIXED : wrong scale of model_size for ext.visual.get_model_summary() ; MODIFIED : add stride arg for ShuffleUnit_Stack and ShuffleUnit_v2_Stack ; add fusion_mode arg for ShuffleUnit_Stack ; improve their documentation. version 0.17.15 [11-16-2018] NEW : add ext.visual sub-module, containing model summarization & visualization toolkits. version 0.17.14 [11-15-2018] FIXED : remove redundant bn5 layer of ShuffleUnit_v2 when stride = 1 and batchnorm_mode = 2. version 0.17.13 [11-13-2018] NEW : add model.Alternate_2D_LSTM module for 2D LSTM implementaton by alternating 1D LSTM along different input dimensions. version 0.17.12 [11-6-2018] NEW : add LSTM2D module for 2D LSTM implementation NEW : add .todevice() interface to Module class for possible support of model-parallel multi-GPU training. However due to Theano issue 6655 , this feature won't be finished, so use it at your own risk. MODIFIED : activation param of Sequential class now supports list input. MODIFIED : merge pull request #1 , #2 , now functional.spatial_pyramid_pooling() supports 3 different implementations. version 0.17.11 [9-3-2018] MODIFIED : returned bbox 's shape is changed to (B, H, W, k, n) for model_CTPN version 0.17.10 [8-16-2018] MODIFIED : add border_mode arg to dandelion.ext.CV.imrotate() , the interpolation arg type is changed to string. MODIFIED : returned bbox 's shape is changed to (B, H, W, n*k) for model_CTPN version 0.17.9 [8-7-2018] NEW : add theano_safe_run, Finite_Memory_Array, pad_sequence_into_array, get_local_ip, get_time_stamp into dandelion.util NEW : add documentation of dandelion.util , unfinished. version 0.17.8 [8-3-2018] MODIFIED : disable the auto-broadcasting in create_param() . This auto-broadcasting would result in theano exception for parameter with shape = [1]. NEW : add model.shufflenet.ShuffleUnit_v2_Stack and model.shufflenet.ShuffleNet_v2 for ShuffleNet_v2 reference implementation. NEW : move channel_shuffle() from model.shufflenet.py into functional.py version 0.17.7 [8-2-2018] From this version the documentaiton supports latex math officially. * MODIFIED : move arg alpha of Module.Center from class delcaration to its .forward() interface. version 0.17.6 [7-25-2018] MODIFIED : change default value of Module.set_weights_by_name() 's arg unmatched from ignore to raise MODIFIED : change default value of model.vgg.model_VGG16() 's arg flip_filters from True to False version 0.17.5 [7-20-2018] FIXED : fixed typo in objective.categorical_crossentropy() version 0.17.4 [7-20-2018] NEW : add class weighting support for objective.categorical_crossentropy() and objective.categorical_crossentropy_log() NEW : add util.theano_safe_run() to help catch memory exceptions when running theano functions. version 0.17.3 [7-18-2018] FIXED : pooling mode in model.shufflenet.ShuffleUnit changed to average_inc_pad for correct gradient. version 0.17.2 [7-17-2018] NEW : add model.shufflenet.model_ShuffleSeg for Shuffle-Seg model reference implementation. version 0.17.1 [7-12-2018] MODIFIED : modify all Test/Test_*.py to be compatible with pytest. NEW : add Travis CI for automatic unit test. version 0.17.0 [7-12-2018] In this version the Module 's parameter and sub-module naming conventions are changed to make sure unique name for each variable/module in a complex network. It's incompatible with previous version if your work accessed their names, otherwise there is no impact. Note: to set weights saved by previous dandelion(>=version 0.14.0), use .set_weights() instead of .set_weights_by_name() . For weights saved by dandelion of version < 0.14.0, the quick way is to set the model's submodule weight explicitly as model_new_dandelion.conv1.W.set_value(model_old_dandelion.conv1.W.get_value()) . From this version, it's recommonded to let the framework auto-name the module parameters when you define your own module with register_param() and register_self_updating_variable() . MODIFIED : module's variable name convention changed to variable_name@parent_module_name to make sure unique name for each variable in a complex network MODIFIED : module's name convention changed to class_name|instance_name@parent_module_name to make sure unique name for each module in a complex network MODIFIED : remove all specified names for register_param() and register_self_updating_variable() . Leave the variables to be named automatically by their parent module. MODIFIED : improve model.shufflenet.ShuffleUnit . NEW : add Sequential container in dandelion.module for usage convenience. NEW : add model.shufflenet.ShuffleUnit_Stack and model.shufflenet.ShuffleNet for ShuffleNet reference implementation. version 0.16.10 [7-10-2018] MODIFIED : disable all install requirements to prevent possible conflict of pip and conda channel. version 0.16.9 [7-10-2018] MODIFIED : import all model reference implementations into dandelion.model 's namespace FIXED : ConvTransposed2D 's W_shape should use in_channels as first dimension; incorrect W_shape when num_groups > 1. version 0.16.8 [7-9-2018] NEW : add model.shufflenet.DSConv2D for Depthwise Separable Convolution reference implementation. NEW : add model.shufflenet.ShuffleUnit for ShuffleNet reference implementation FIXED : W_shape of module.Conv2D should count for num_groups version 0.16.7 [7-6-2018] NEW : add model.vgg.model_VGG16 for VGG-16 reference implementation. NEW : add model.resnet.ResNet_bottleneck for ResNet reference implementation NEW : add model.feature_pyramid_net.model_FPN for Feature Pyramid Network reference implementation version 0.16.6 [7-5-2018] NEW : add functional.upsample_2d() for 2D upsampling NEW : add functional.upsample_2d_bilinear() for bilinear 2D upsampling version 0.16.5 [7-5-2018] NEW : add functional.spatial_pyramid_pooling() for SPP-net implementation. version 0.16.4 [7-4-2018] FIXED : wrong indexing when targets is int vector for objective.categorical_crossentropy_log() . version 0.16.3 [7-3-2018] NEW : add activation.log_softmax() for more numerically stable softmax. NEW : add objective.categorical_crossentropy_log() for more numerically stable categorical cross-entropy MODIFIED : add eps argument to objective.categorical_crossentropy() for numerical stability purpose. Note 1e-7 is set as default value of eps . You can set it to 0 to get the old categorical_crossentropy() back. version 0.16.0 [6-13-2018] NEW : add ext module into master branch of Dandelion. All the miscellaneous extensions will be organized in here. NEW : add ext.CV sub-module, containing image I/O functions and basic image processing functions commonly used in model training. version 0.15.2 [5-28-2018] FIXED : convTOP should be constructed each time the forward() function of ConvTransposed2D is called. version 0.15.1 [5-25-2018] NEW : add model module into master branch of Dandelion NEW : add U-net FCN implementation into model module NEW : add align_crop() into functional module version 0.14.4 [4-17-2018] Rename updates.py with update.py version 0.14.0 [4-10-2018] In this version the Module 's parameter interfaces are mostly redesigned, so it's incompatible with previous version. Now self.params and self.self_updating_variables do not include sub-modules' parameters any more, to get all the parameters to be trained by optimizer, including sub-modules' during training, you'll need to call the new interface function .collect_params() . To collect self-defined updates for training, still call .collect_self_updates() . MODIFIED : .get_weights() and .set_weights() traverse the parameters in the same order of sub-modules, so they're incompatible with previous version. MODIFIED : Rewind all trainable flags, you're now expected to use the include and exclude arguments in .collect_params() and .collect_self_updates() to enable/disable training for certain module's parameters. MODIFIED : to define self-update expression for self_updating_variable , use .update attribute instead of previous .default_update NEW : add auto-naming feature to root class Module : if a sub-module is unnamed yet, it'll be auto-named by its instance name, from now on you don't need to name a sub-module manually any more. NEW : add .set_weights_by_name() to Module class, you can use this function to set module weights saved by previous version of Dandelion","title":"History"},{"location":"history/#history","text":"","title":"History"},{"location":"history/#version-01719-1-23-2019","text":"NEW : add clear_nan argument for sgd , adam and adadelta optimizers. MODIFIED : add default value 1e-4 for sgd 's learning_rate arg.","title":"version 0.17.19 [1-23-2019]"},{"location":"history/#version-01717-11-22-2018","text":"NEW : add GroupNorm module for group normalization implementation; MODIFIED : expose dim_broadcast arg for Module.register_param() method; MODIFIED : replace spec = tensor.patternbroadcast(spec, dim_broadcast) with spec = theano.shared(spec, broadcastable=dim_broadcast) for util.create_param() , due to the former would change tensor's type.","title":"version 0.17.17 [11-22-2018]"},{"location":"history/#version-01716-11-19-2018","text":"FIXED : wrong scale of model_size for ext.visual.get_model_summary() ; MODIFIED : add stride arg for ShuffleUnit_Stack and ShuffleUnit_v2_Stack ; add fusion_mode arg for ShuffleUnit_Stack ; improve their documentation.","title":"version 0.17.16 [11-19-2018]"},{"location":"history/#version-01715-11-16-2018","text":"NEW : add ext.visual sub-module, containing model summarization & visualization toolkits.","title":"version 0.17.15 [11-16-2018]"},{"location":"history/#version-01714-11-15-2018","text":"FIXED : remove redundant bn5 layer of ShuffleUnit_v2 when stride = 1 and batchnorm_mode = 2.","title":"version 0.17.14 [11-15-2018]"},{"location":"history/#version-01713-11-13-2018","text":"NEW : add model.Alternate_2D_LSTM module for 2D LSTM implementaton by alternating 1D LSTM along different input dimensions.","title":"version 0.17.13 [11-13-2018]"},{"location":"history/#version-01712-11-6-2018","text":"NEW : add LSTM2D module for 2D LSTM implementation NEW : add .todevice() interface to Module class for possible support of model-parallel multi-GPU training. However due to Theano issue 6655 , this feature won't be finished, so use it at your own risk. MODIFIED : activation param of Sequential class now supports list input. MODIFIED : merge pull request #1 , #2 , now functional.spatial_pyramid_pooling() supports 3 different implementations.","title":"version 0.17.12 [11-6-2018]"},{"location":"history/#version-01711-9-3-2018","text":"MODIFIED : returned bbox 's shape is changed to (B, H, W, k, n) for model_CTPN","title":"version 0.17.11 [9-3-2018]"},{"location":"history/#version-01710-8-16-2018","text":"MODIFIED : add border_mode arg to dandelion.ext.CV.imrotate() , the interpolation arg type is changed to string. MODIFIED : returned bbox 's shape is changed to (B, H, W, n*k) for model_CTPN","title":"version 0.17.10 [8-16-2018]"},{"location":"history/#version-0179-8-7-2018","text":"NEW : add theano_safe_run, Finite_Memory_Array, pad_sequence_into_array, get_local_ip, get_time_stamp into dandelion.util NEW : add documentation of dandelion.util , unfinished.","title":"version 0.17.9 [8-7-2018]"},{"location":"history/#version-0178-8-3-2018","text":"MODIFIED : disable the auto-broadcasting in create_param() . This auto-broadcasting would result in theano exception for parameter with shape = [1]. NEW : add model.shufflenet.ShuffleUnit_v2_Stack and model.shufflenet.ShuffleNet_v2 for ShuffleNet_v2 reference implementation. NEW : move channel_shuffle() from model.shufflenet.py into functional.py","title":"version 0.17.8 [8-3-2018]"},{"location":"history/#version-0177-8-2-2018","text":"From this version the documentaiton supports latex math officially. * MODIFIED : move arg alpha of Module.Center from class delcaration to its .forward() interface.","title":"version 0.17.7 [8-2-2018]"},{"location":"history/#version-0176-7-25-2018","text":"MODIFIED : change default value of Module.set_weights_by_name() 's arg unmatched from ignore to raise MODIFIED : change default value of model.vgg.model_VGG16() 's arg flip_filters from True to False","title":"version 0.17.6 [7-25-2018]"},{"location":"history/#version-0175-7-20-2018","text":"FIXED : fixed typo in objective.categorical_crossentropy()","title":"version 0.17.5 [7-20-2018]"},{"location":"history/#version-0174-7-20-2018","text":"NEW : add class weighting support for objective.categorical_crossentropy() and objective.categorical_crossentropy_log() NEW : add util.theano_safe_run() to help catch memory exceptions when running theano functions.","title":"version 0.17.4 [7-20-2018]"},{"location":"history/#version-0173-7-18-2018","text":"FIXED : pooling mode in model.shufflenet.ShuffleUnit changed to average_inc_pad for correct gradient.","title":"version 0.17.3 [7-18-2018]"},{"location":"history/#version-0172-7-17-2018","text":"NEW : add model.shufflenet.model_ShuffleSeg for Shuffle-Seg model reference implementation.","title":"version 0.17.2 [7-17-2018]"},{"location":"history/#version-0171-7-12-2018","text":"MODIFIED : modify all Test/Test_*.py to be compatible with pytest. NEW : add Travis CI for automatic unit test.","title":"version 0.17.1 [7-12-2018]"},{"location":"history/#version-0170-7-12-2018","text":"In this version the Module 's parameter and sub-module naming conventions are changed to make sure unique name for each variable/module in a complex network. It's incompatible with previous version if your work accessed their names, otherwise there is no impact. Note: to set weights saved by previous dandelion(>=version 0.14.0), use .set_weights() instead of .set_weights_by_name() . For weights saved by dandelion of version < 0.14.0, the quick way is to set the model's submodule weight explicitly as model_new_dandelion.conv1.W.set_value(model_old_dandelion.conv1.W.get_value()) . From this version, it's recommonded to let the framework auto-name the module parameters when you define your own module with register_param() and register_self_updating_variable() . MODIFIED : module's variable name convention changed to variable_name@parent_module_name to make sure unique name for each variable in a complex network MODIFIED : module's name convention changed to class_name|instance_name@parent_module_name to make sure unique name for each module in a complex network MODIFIED : remove all specified names for register_param() and register_self_updating_variable() . Leave the variables to be named automatically by their parent module. MODIFIED : improve model.shufflenet.ShuffleUnit . NEW : add Sequential container in dandelion.module for usage convenience. NEW : add model.shufflenet.ShuffleUnit_Stack and model.shufflenet.ShuffleNet for ShuffleNet reference implementation.","title":"version 0.17.0 [7-12-2018]"},{"location":"history/#version-01610-7-10-2018","text":"MODIFIED : disable all install requirements to prevent possible conflict of pip and conda channel.","title":"version 0.16.10 [7-10-2018]"},{"location":"history/#version-0169-7-10-2018","text":"MODIFIED : import all model reference implementations into dandelion.model 's namespace FIXED : ConvTransposed2D 's W_shape should use in_channels as first dimension; incorrect W_shape when num_groups > 1.","title":"version 0.16.9 [7-10-2018]"},{"location":"history/#version-0168-7-9-2018","text":"NEW : add model.shufflenet.DSConv2D for Depthwise Separable Convolution reference implementation. NEW : add model.shufflenet.ShuffleUnit for ShuffleNet reference implementation FIXED : W_shape of module.Conv2D should count for num_groups","title":"version 0.16.8 [7-9-2018]"},{"location":"history/#version-0167-7-6-2018","text":"NEW : add model.vgg.model_VGG16 for VGG-16 reference implementation. NEW : add model.resnet.ResNet_bottleneck for ResNet reference implementation NEW : add model.feature_pyramid_net.model_FPN for Feature Pyramid Network reference implementation","title":"version 0.16.7 [7-6-2018]"},{"location":"history/#version-0166-7-5-2018","text":"NEW : add functional.upsample_2d() for 2D upsampling NEW : add functional.upsample_2d_bilinear() for bilinear 2D upsampling","title":"version 0.16.6 [7-5-2018]"},{"location":"history/#version-0165-7-5-2018","text":"NEW : add functional.spatial_pyramid_pooling() for SPP-net implementation.","title":"version 0.16.5 [7-5-2018]"},{"location":"history/#version-0164-7-4-2018","text":"FIXED : wrong indexing when targets is int vector for objective.categorical_crossentropy_log() .","title":"version 0.16.4 [7-4-2018]"},{"location":"history/#version-0163-7-3-2018","text":"NEW : add activation.log_softmax() for more numerically stable softmax. NEW : add objective.categorical_crossentropy_log() for more numerically stable categorical cross-entropy MODIFIED : add eps argument to objective.categorical_crossentropy() for numerical stability purpose. Note 1e-7 is set as default value of eps . You can set it to 0 to get the old categorical_crossentropy() back.","title":"version 0.16.3 [7-3-2018]"},{"location":"history/#version-0160-6-13-2018","text":"NEW : add ext module into master branch of Dandelion. All the miscellaneous extensions will be organized in here. NEW : add ext.CV sub-module, containing image I/O functions and basic image processing functions commonly used in model training.","title":"version 0.16.0 [6-13-2018]"},{"location":"history/#version-0152-5-28-2018","text":"FIXED : convTOP should be constructed each time the forward() function of ConvTransposed2D is called.","title":"version 0.15.2 [5-28-2018]"},{"location":"history/#version-0151-5-25-2018","text":"NEW : add model module into master branch of Dandelion NEW : add U-net FCN implementation into model module NEW : add align_crop() into functional module","title":"version 0.15.1 [5-25-2018]"},{"location":"history/#version-0144-4-17-2018","text":"Rename updates.py with update.py","title":"version 0.14.4 [4-17-2018]"},{"location":"history/#version-0140-4-10-2018","text":"In this version the Module 's parameter interfaces are mostly redesigned, so it's incompatible with previous version. Now self.params and self.self_updating_variables do not include sub-modules' parameters any more, to get all the parameters to be trained by optimizer, including sub-modules' during training, you'll need to call the new interface function .collect_params() . To collect self-defined updates for training, still call .collect_self_updates() . MODIFIED : .get_weights() and .set_weights() traverse the parameters in the same order of sub-modules, so they're incompatible with previous version. MODIFIED : Rewind all trainable flags, you're now expected to use the include and exclude arguments in .collect_params() and .collect_self_updates() to enable/disable training for certain module's parameters. MODIFIED : to define self-update expression for self_updating_variable , use .update attribute instead of previous .default_update NEW : add auto-naming feature to root class Module : if a sub-module is unnamed yet, it'll be auto-named by its instance name, from now on you don't need to name a sub-module manually any more. NEW : add .set_weights_by_name() to Module class, you can use this function to set module weights saved by previous version of Dandelion","title":"version 0.14.0 [4-10-2018]"},{"location":"howtos/","text":"Tutorial III: Howtos 1) How to freeze a module during training? To freeze a module during training, use the include and exclude arguments of module's .collect_params() and .collect_self_updates() functions. Example class FOO(Module): def __init__(self): self.cnn0 = Conv2D(...) self.cnn1 = Conv2D(...) self.cnn2 = Conv2D(...) .... # Now we will freeze cnn0 and cnn1 submodules during training model = Foo() loss = ... params = model.collect_params(exclude=['cnn0', 'cnn1']) updates = optimizer(loss, params) updates.update(model.colect_self_updates(exclude=['cnn0', 'cnn1'])) train_fn = theano.function([...], [...], updates=updates, no_default_updates=False) 2) How to initialize a partially modified model with previouslly trained weights? A frequently encountered scenario in research is that we want to re-use trained weights from a previous model to initialize a new model, usually partially modified. The most convenient way is to use Module 's set_weights_by_name() method with the unmatched argument set to warn or ignore . To use this method, it's assumed that you didn't change the variable's name to be initialized; otherwise, you can use the name_map argument to input the corresponding weight->variable mapping, or the most primitive way, use tensor's get_value() and set_value() methods explicitly. Example from dandelion.util import gpickle old_model_file = ... old_module_weights, old_userdata = gpickle.load(old_model_file) new_model = ... new_model.set_weights_by_name(old_module_weights, unmatched='warn') 3) How to add random noise to a tensor? Just use Theano's MRG_RandomStreams module. Example from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams srng = RandomStreams(np.random.randint(1, 2147462579)) .... y = x + srng.normal(x.shape, avg=0.0, std=0.1) # add Gaussian noise to x What you'd keep in mind is that if you used Theano's MRG_RandomStreams module, remember to set no_default_updates=False when compiling functions. 4) How to do model-parallel training? According to issue 6655 , model-parallel multi-GPU support of Theano will never be finished, so it won't be possible to do model-parallel training with Theano, and of course, Dandelion. For data-parallel training, refer to platoon for possible solution. We may implement our multi-GPU data-parallel training scheme later, stay tuned.","title":"III - Howtos"},{"location":"howtos/#tutorial-iii-howtos","text":"","title":"Tutorial III: Howtos"},{"location":"howtos/#1-how-to-freeze-a-module-during-training","text":"To freeze a module during training, use the include and exclude arguments of module's .collect_params() and .collect_self_updates() functions.","title":"1) How to freeze a module during training?"},{"location":"howtos/#example","text":"class FOO(Module): def __init__(self): self.cnn0 = Conv2D(...) self.cnn1 = Conv2D(...) self.cnn2 = Conv2D(...) .... # Now we will freeze cnn0 and cnn1 submodules during training model = Foo() loss = ... params = model.collect_params(exclude=['cnn0', 'cnn1']) updates = optimizer(loss, params) updates.update(model.colect_self_updates(exclude=['cnn0', 'cnn1'])) train_fn = theano.function([...], [...], updates=updates, no_default_updates=False)","title":"Example"},{"location":"howtos/#2-how-to-initialize-a-partially-modified-model-with-previouslly-trained-weights","text":"A frequently encountered scenario in research is that we want to re-use trained weights from a previous model to initialize a new model, usually partially modified. The most convenient way is to use Module 's set_weights_by_name() method with the unmatched argument set to warn or ignore . To use this method, it's assumed that you didn't change the variable's name to be initialized; otherwise, you can use the name_map argument to input the corresponding weight->variable mapping, or the most primitive way, use tensor's get_value() and set_value() methods explicitly.","title":"2) How to initialize a partially modified model with previouslly trained weights?"},{"location":"howtos/#example_1","text":"from dandelion.util import gpickle old_model_file = ... old_module_weights, old_userdata = gpickle.load(old_model_file) new_model = ... new_model.set_weights_by_name(old_module_weights, unmatched='warn')","title":"Example"},{"location":"howtos/#3-how-to-add-random-noise-to-a-tensor","text":"Just use Theano's MRG_RandomStreams module.","title":"3) How to add random noise to a tensor?"},{"location":"howtos/#example_2","text":"from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams srng = RandomStreams(np.random.randint(1, 2147462579)) .... y = x + srng.normal(x.shape, avg=0.0, std=0.1) # add Gaussian noise to x What you'd keep in mind is that if you used Theano's MRG_RandomStreams module, remember to set no_default_updates=False when compiling functions.","title":"Example"},{"location":"howtos/#4-how-to-do-model-parallel-training","text":"According to issue 6655 , model-parallel multi-GPU support of Theano will never be finished, so it won't be possible to do model-parallel training with Theano, and of course, Dandelion. For data-parallel training, refer to platoon for possible solution. We may implement our multi-GPU data-parallel training scheme later, stay tuned.","title":"4) How to do model-parallel training?"},{"location":"tutorial I - Sentence Topic Classification/","text":"Tutorial I: Sentence topic classification The best way to understand how Dandelion works is through practical examples. In the first part of this tutorial, you\u2019ll be guided through model definition and train/test/predict function compiling with a practical sentence classification task. Sentence Classification Task Objective: classify each sentence into different topic categories. Variant: single-tag classification vs multi-tag classification The sentence classification task is to using neural network model to determine the topic of each sentence, i.e., what each sentence is talking about. For example: time, location, cause, action and result. To fulfill the task, we\u2019ll build a model basically based on RNN, LSTM specifically. Model Definition - Modules For the full model definition, check the following code snippet: import theano import theano.tensor as tensor from dandelion.module import * from dandelion.update import * from dandelion.functional import * from dandelion.util import gpickle class model(Module): def __init__(self, batchsize=None, input_length=None, Nclass=6, noise=(0.5, 0.2, 0.7, 0.7, 0.7)): super().__init__() self.batchsize = batchsize self.input_length = input_length self.Nclass = Nclass self.noise = noise self.dropout0 = Dropout(name='dropout0') self.dropout1 = Dropout(name='dropout1') self.dropout2 = Dropout(name='dropout2') self.dropout3 = Dropout(name='dropout3') self.dropout4 = Dropout(name='dropout4') W = gpickle.load('word_embedding(6336, 256).gpkl') self.embedding = Embedding(num_embeddings=6336, embedding_dim=256, W=W, name='Embedding') self.lstm0 = LSTM(input_dims=256, hidden_dim=100, name='lstm0') self.lstm1 = LSTM(input_dims=256, hidden_dim=100, name='lstm1') self.lstm2 = LSTM(input_dims=200, hidden_dim=100, name='lstm2') self.lstm3 = LSTM(input_dims=200, hidden_dim=100, name='lstm3') self.lstm4 = LSTM(input_dims=200, hidden_dim=100, name='lstm4') self.lstm5 = LSTM(input_dims=200, hidden_dim=100, name='lstm5') self.dense = Dense(input_dims=200, output_dim=Nclass, name='dense') All the neural network modules are defined in dandelion.module in Python. For the sentence classification task, the following four NN modules will be used: Dropout , Embedding , LSTM and Dense . To define our model, we\u2019ll need to subclass the Module class from dandelion.module . The Module class is the base class for all our NN modules. There\u2019s no complex abstraction here, all Module class done is to define some convenient interfaces for model parameter manipulation and no more. The Module class is quite similar with Pytorch\u2019s nn.Module class. Now we define all the network modules as our model\u2019s attributes, such as self.dropout0 = Dropout(name='dropout0') You can drop the name here, it\u2019s optional. However for possible parameter manipulation convenience later, we\u2019d suggest giving a unique name for each network module here. (After version 0.14.0, you don't need to set the module name manually any more, they will be auto-named by the sub-module keys) Note that all these definitions are done in the model \u2019s __init__() part. Now we defined all the NN modules to be used in our model, but their relations, i.e., the network structure hasn\u2019t been done. This part will be defined in model\u2019s forward() and predict() functions later. If you\u2019re familiar with Lasagne or Keras, you\u2019d notice that for LSTM module, Dandelion requires both the input dimension via input_dims and output dimension via hidden_dim meanwhile Lasagne or Keras would only require the output dimension, leaving the input dimension determined automatically by the framework. This is the cost you\u2019d pay for greater flexibility by using Dandelion. Model Definition - Structures Now we\u2019ll go through the network structure part. Usually a model needs to be trained first then it can be used in inference, so the network structure would involve these two different processes, i.e., training and inference. We define the network structure for training in Model \u2019s forward() function, as showed below. def forward(self, x): self.work_mode = 'train' x = self.dropout0.forward(x, p=self.noise[0], rescale=False) x = self.embedding.forward(x) # (B, T, D) x = self.dropout1.forward(x, p=self.noise[1], rescale=True) x = x.dimshuffle((1, 0, 2)) # (B, T, D) -> (T, B, D) x_f = self.lstm0.forward(x, None, None, None) x_b = self.lstm1.forward(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0) x = self.dropout2.forward(x, p=self.noise[2], rescale=True) x_f = self.lstm2.forward(x, None, None, None) x_b = self.lstm3.forward(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x = self.dropout3.forward(x, p=self.noise[3], rescale=True) x_f = self.lstm4.forward(x, None, None, None, only_return_final=True) x_b = self.lstm5.forward(x, None, None, None, only_return_final=True, backward=True) x = tensor.concatenate([x_f, x_b], axis=1) x = self.dropout4.forward(x, p=self.noise[4], rescale=True) y = sigmoid(self.dense.forward(x)) return y Within the forward() function, we first set the work mode to train . This is an optional step, which will be explained later. Then the input text sequence is fed through a Dropout and Embedding module to convert integer indices into character embedding vectors. After that are two LSTM modules with forward and backward scanning directions, resulting in a bidirectional LSTM. Output of this bi-LSTM is then subsampled along the time dimension, and then fed into another bi-LSTM. Note that for the latter bi-LSTM, we only need the last time frame as output. Finally a Dense module followed by a sigmoid activation gives the sentence classification result. The network structure can be plotted as Here the five Dropout modules are plotted with green color, means they only exist during training process. def predict(self, x): self.work_mode = 'inference' x = self.embedding.predict(x) x = x.dimshuffle((1, 0, 2)) # (B, T, D) -> (T, B, D) x_f = self.lstm0.predict(x, None, None, None) x_b = self.lstm1.predict(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0) x_f = self.lstm2.predict(x, None, None, None) x_b = self.lstm3.predict(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x_f = self.lstm4.predict(x, None, None, None, only_return_final=True) x_b = self.lstm5.predict(x, None, None, None, only_return_final=True, backward=True) x = tensor.concatenate([x_f, x_b], axis=1) y = sigmoid(self.dense.predict(x)) return y Now we define the network structure for inference in model \u2019s predict() function, as showed above. During inference process, the model \u2019s network structure is simpler than in training. Note that there\u2019s no Dropout modules here. The rest part of the predict() function is quite the same with forward() function, except that now all the modules\u2019 predict() interface are called instead of the forward() interface as in model \u2019s forward() function. Unified Calling Interface For users familiar with Keras or Lasagne, you might be confused that we define separate functions for both training and inference. In Keras/Lasagne, the common way is to define the model\u2019s structure and use a flag parameter to tell the model to work in training mode or in inference mode. The reason we do this is because it allows us to use different network structures for different purpose, i.e., the model\u2019s network structure for training can be quite different from the structure for inference. However the cost of this flexibility is that we\u2019d have to define the network structure twice even though in most scenarios the model\u2019s network structure is the same for both training and inference. Fortunately we\u2019ve considered this and provide a unified calling interface in Dandelion. For the network structures defined before, they can be re-written by the unified calling interface as follows def call(self, x, work_mode='train'): self.work_mode = work_mode x = self.dropout0(x, p=self.noise[0], rescale=False) x = self.embedding(x) # (B, T, D) x = self.dropout1(x, p=self.noise[1], rescale=True) x = x.dimshuffle((1, 0, 2)) # (B, T, D) -> (T, B, D) x_f = self.lstm0(x, None, None, None) x_b = self.lstm1(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0) x = self.dropout2(x, p=self.noise[2], rescale=True) x_f = self.lstm2(x, None, None, None) x_b = self.lstm3(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x = self.dropout3(x, p=self.noise[3], rescale=True) x_f = self.lstm4(x, None, None, None, only_return_final=True) x_b = self.lstm5(x, None, None, None, only_return_final=True, backward=True) x = tensor.concatenate([x_f, x_b], axis=1) x = self.dropout4(x, p=self.noise[4], rescale=True) y = sigmoid(self.dense(x)) return y As we can see from the code above, now we do not call the sub-module\u2019s forward() or predict() interface anymore. By setting the work_mode parameter, Dandelion will automatically call the sub-module\u2019s forward() or predict() interface accordingly. Now we only need define the network structure for once, and use it for both training and inference. Model Compiling Theano requires compiling the computation graph before using it. The model compiling is actually more relevant to Theano than to Dandelion. print(' compiling train func') X = tensor.imatrix('X') Y = tensor.fmatrix('Y') output_score = model.forward(X) B = Y.shape[0] B = tensor.cast(B, 'float32') loss = tensor.sqrt(tensor.sum((output_score - Y)**2)) / B * 100 Y_out_positive = tensor.zeros_like(output_score) Y_out_positive = tensor.switch(output_score>0.5, Y, Y_out_positive) acc_positive = tensor.sum(Y_out_positive) / tensor.sum(Y) Y_out_negative = tensor.zeros_like(output_score) Y_out_negative = tensor.switch(output_score<=0.5, 1.0 - Y, Y_out_negative) acc_negative = tensor.sum(Y_out_negative) / tensor.sum(1.0- Y) params = model.collect_params() updates = adadelta(loss, params) updates.update(model.collect_self_updates()) train_fn = theano.function([X, Y], [loss, acc_positive, acc_negative], updates=updates, no_default_updates=False) Model Calling Here we call the model defined before by model.forward() . Of course you can also call the model by the unified calling interface as model.call(\u2026, work_mode=\u2018train\u2019) Parameters Collecting Parameters to be trained by optimizer can be collected from the model by calling model.collect_params() , simply like that. Updates Collecting In Dandelion, there\u2019re two kinds of parameters: parameters to be updated by optimizer and parameters to be updated by other methods. The updates expression of the latter part of parameters can be collected by calling model.collect_self_updates() . Returned is a dict describing updates for each parameter accordingly. After these 3 steps, now we can compile the training function by Theano simply by train_fn = theano.function([X, Y], [loss, acc_positive, acc_negative], updates=updates, no_default_updates=False)","title":"I - Sentence Topic Classification"},{"location":"tutorial I - Sentence Topic Classification/#tutorial-i-sentence-topic-classification","text":"The best way to understand how Dandelion works is through practical examples. In the first part of this tutorial, you\u2019ll be guided through model definition and train/test/predict function compiling with a practical sentence classification task.","title":"Tutorial I: Sentence topic classification"},{"location":"tutorial I - Sentence Topic Classification/#sentence-classification-task","text":"Objective: classify each sentence into different topic categories. Variant: single-tag classification vs multi-tag classification The sentence classification task is to using neural network model to determine the topic of each sentence, i.e., what each sentence is talking about. For example: time, location, cause, action and result. To fulfill the task, we\u2019ll build a model basically based on RNN, LSTM specifically.","title":"Sentence Classification Task"},{"location":"tutorial I - Sentence Topic Classification/#model-definition-modules","text":"For the full model definition, check the following code snippet: import theano import theano.tensor as tensor from dandelion.module import * from dandelion.update import * from dandelion.functional import * from dandelion.util import gpickle class model(Module): def __init__(self, batchsize=None, input_length=None, Nclass=6, noise=(0.5, 0.2, 0.7, 0.7, 0.7)): super().__init__() self.batchsize = batchsize self.input_length = input_length self.Nclass = Nclass self.noise = noise self.dropout0 = Dropout(name='dropout0') self.dropout1 = Dropout(name='dropout1') self.dropout2 = Dropout(name='dropout2') self.dropout3 = Dropout(name='dropout3') self.dropout4 = Dropout(name='dropout4') W = gpickle.load('word_embedding(6336, 256).gpkl') self.embedding = Embedding(num_embeddings=6336, embedding_dim=256, W=W, name='Embedding') self.lstm0 = LSTM(input_dims=256, hidden_dim=100, name='lstm0') self.lstm1 = LSTM(input_dims=256, hidden_dim=100, name='lstm1') self.lstm2 = LSTM(input_dims=200, hidden_dim=100, name='lstm2') self.lstm3 = LSTM(input_dims=200, hidden_dim=100, name='lstm3') self.lstm4 = LSTM(input_dims=200, hidden_dim=100, name='lstm4') self.lstm5 = LSTM(input_dims=200, hidden_dim=100, name='lstm5') self.dense = Dense(input_dims=200, output_dim=Nclass, name='dense') All the neural network modules are defined in dandelion.module in Python. For the sentence classification task, the following four NN modules will be used: Dropout , Embedding , LSTM and Dense . To define our model, we\u2019ll need to subclass the Module class from dandelion.module . The Module class is the base class for all our NN modules. There\u2019s no complex abstraction here, all Module class done is to define some convenient interfaces for model parameter manipulation and no more. The Module class is quite similar with Pytorch\u2019s nn.Module class. Now we define all the network modules as our model\u2019s attributes, such as self.dropout0 = Dropout(name='dropout0') You can drop the name here, it\u2019s optional. However for possible parameter manipulation convenience later, we\u2019d suggest giving a unique name for each network module here. (After version 0.14.0, you don't need to set the module name manually any more, they will be auto-named by the sub-module keys) Note that all these definitions are done in the model \u2019s __init__() part. Now we defined all the NN modules to be used in our model, but their relations, i.e., the network structure hasn\u2019t been done. This part will be defined in model\u2019s forward() and predict() functions later. If you\u2019re familiar with Lasagne or Keras, you\u2019d notice that for LSTM module, Dandelion requires both the input dimension via input_dims and output dimension via hidden_dim meanwhile Lasagne or Keras would only require the output dimension, leaving the input dimension determined automatically by the framework. This is the cost you\u2019d pay for greater flexibility by using Dandelion.","title":"Model Definition - Modules"},{"location":"tutorial I - Sentence Topic Classification/#model-definition-structures","text":"Now we\u2019ll go through the network structure part. Usually a model needs to be trained first then it can be used in inference, so the network structure would involve these two different processes, i.e., training and inference. We define the network structure for training in Model \u2019s forward() function, as showed below. def forward(self, x): self.work_mode = 'train' x = self.dropout0.forward(x, p=self.noise[0], rescale=False) x = self.embedding.forward(x) # (B, T, D) x = self.dropout1.forward(x, p=self.noise[1], rescale=True) x = x.dimshuffle((1, 0, 2)) # (B, T, D) -> (T, B, D) x_f = self.lstm0.forward(x, None, None, None) x_b = self.lstm1.forward(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0) x = self.dropout2.forward(x, p=self.noise[2], rescale=True) x_f = self.lstm2.forward(x, None, None, None) x_b = self.lstm3.forward(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x = self.dropout3.forward(x, p=self.noise[3], rescale=True) x_f = self.lstm4.forward(x, None, None, None, only_return_final=True) x_b = self.lstm5.forward(x, None, None, None, only_return_final=True, backward=True) x = tensor.concatenate([x_f, x_b], axis=1) x = self.dropout4.forward(x, p=self.noise[4], rescale=True) y = sigmoid(self.dense.forward(x)) return y Within the forward() function, we first set the work mode to train . This is an optional step, which will be explained later. Then the input text sequence is fed through a Dropout and Embedding module to convert integer indices into character embedding vectors. After that are two LSTM modules with forward and backward scanning directions, resulting in a bidirectional LSTM. Output of this bi-LSTM is then subsampled along the time dimension, and then fed into another bi-LSTM. Note that for the latter bi-LSTM, we only need the last time frame as output. Finally a Dense module followed by a sigmoid activation gives the sentence classification result. The network structure can be plotted as Here the five Dropout modules are plotted with green color, means they only exist during training process. def predict(self, x): self.work_mode = 'inference' x = self.embedding.predict(x) x = x.dimshuffle((1, 0, 2)) # (B, T, D) -> (T, B, D) x_f = self.lstm0.predict(x, None, None, None) x_b = self.lstm1.predict(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0) x_f = self.lstm2.predict(x, None, None, None) x_b = self.lstm3.predict(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x_f = self.lstm4.predict(x, None, None, None, only_return_final=True) x_b = self.lstm5.predict(x, None, None, None, only_return_final=True, backward=True) x = tensor.concatenate([x_f, x_b], axis=1) y = sigmoid(self.dense.predict(x)) return y Now we define the network structure for inference in model \u2019s predict() function, as showed above. During inference process, the model \u2019s network structure is simpler than in training. Note that there\u2019s no Dropout modules here. The rest part of the predict() function is quite the same with forward() function, except that now all the modules\u2019 predict() interface are called instead of the forward() interface as in model \u2019s forward() function.","title":"Model Definition - Structures"},{"location":"tutorial I - Sentence Topic Classification/#unified-calling-interface","text":"For users familiar with Keras or Lasagne, you might be confused that we define separate functions for both training and inference. In Keras/Lasagne, the common way is to define the model\u2019s structure and use a flag parameter to tell the model to work in training mode or in inference mode. The reason we do this is because it allows us to use different network structures for different purpose, i.e., the model\u2019s network structure for training can be quite different from the structure for inference. However the cost of this flexibility is that we\u2019d have to define the network structure twice even though in most scenarios the model\u2019s network structure is the same for both training and inference. Fortunately we\u2019ve considered this and provide a unified calling interface in Dandelion. For the network structures defined before, they can be re-written by the unified calling interface as follows def call(self, x, work_mode='train'): self.work_mode = work_mode x = self.dropout0(x, p=self.noise[0], rescale=False) x = self.embedding(x) # (B, T, D) x = self.dropout1(x, p=self.noise[1], rescale=True) x = x.dimshuffle((1, 0, 2)) # (B, T, D) -> (T, B, D) x_f = self.lstm0(x, None, None, None) x_b = self.lstm1(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0) x = self.dropout2(x, p=self.noise[2], rescale=True) x_f = self.lstm2(x, None, None, None) x_b = self.lstm3(x, None, None, None, backward=True) x = tensor.concatenate([x_f, x_b], axis=2) x = self.dropout3(x, p=self.noise[3], rescale=True) x_f = self.lstm4(x, None, None, None, only_return_final=True) x_b = self.lstm5(x, None, None, None, only_return_final=True, backward=True) x = tensor.concatenate([x_f, x_b], axis=1) x = self.dropout4(x, p=self.noise[4], rescale=True) y = sigmoid(self.dense(x)) return y As we can see from the code above, now we do not call the sub-module\u2019s forward() or predict() interface anymore. By setting the work_mode parameter, Dandelion will automatically call the sub-module\u2019s forward() or predict() interface accordingly. Now we only need define the network structure for once, and use it for both training and inference.","title":"Unified Calling Interface"},{"location":"tutorial I - Sentence Topic Classification/#model-compiling","text":"Theano requires compiling the computation graph before using it. The model compiling is actually more relevant to Theano than to Dandelion. print(' compiling train func') X = tensor.imatrix('X') Y = tensor.fmatrix('Y') output_score = model.forward(X) B = Y.shape[0] B = tensor.cast(B, 'float32') loss = tensor.sqrt(tensor.sum((output_score - Y)**2)) / B * 100 Y_out_positive = tensor.zeros_like(output_score) Y_out_positive = tensor.switch(output_score>0.5, Y, Y_out_positive) acc_positive = tensor.sum(Y_out_positive) / tensor.sum(Y) Y_out_negative = tensor.zeros_like(output_score) Y_out_negative = tensor.switch(output_score<=0.5, 1.0 - Y, Y_out_negative) acc_negative = tensor.sum(Y_out_negative) / tensor.sum(1.0- Y) params = model.collect_params() updates = adadelta(loss, params) updates.update(model.collect_self_updates()) train_fn = theano.function([X, Y], [loss, acc_positive, acc_negative], updates=updates, no_default_updates=False) Model Calling Here we call the model defined before by model.forward() . Of course you can also call the model by the unified calling interface as model.call(\u2026, work_mode=\u2018train\u2019) Parameters Collecting Parameters to be trained by optimizer can be collected from the model by calling model.collect_params() , simply like that. Updates Collecting In Dandelion, there\u2019re two kinds of parameters: parameters to be updated by optimizer and parameters to be updated by other methods. The updates expression of the latter part of parameters can be collected by calling model.collect_self_updates() . Returned is a dict describing updates for each parameter accordingly. After these 3 steps, now we can compile the training function by Theano simply by train_fn = theano.function([X, Y], [loss, acc_positive, acc_negative], updates=updates, no_default_updates=False)","title":"Model Compiling"},{"location":"tutorial II - Write Your Own Module/","text":"Tutorial II: Write Your Own Module In this tutorial, you\u2019ll learn how to write your own neural network module with the help of Dandelion. Here we\u2019ll design a module which gives the class centers for classification output. It\u2019s a simple case for Dandelion yet not so intuitive for Lasagne or Keras users. In image classification tasks, such as face recognition, document image classification, Imagenet contests, etc., we usually consider only the \u201cpositive\u201d samples, i.e., we assume that given any input sample, it would be associated with at least one out of all the known class labels. However, in actual applications, we often also want the trained neural network model to be able to tell whether an input sample is an \u201coutsider\u201d or not. To accomplish this task, we can add an extra \u201cnegative\u201d class to the final layer of the network, and then train this augmented network by feeding it with all kinds of \u201cnegative\u201d samples you can collect. It\u2019s pure data-driven, so the bottleneck is how many \u201cnegative\u201d samples can be collected. Another way is algorithm-driven: we design a new network module to explore the intrinsic properties of the data, and use these \u201cproperties\u201d to reject or accept an sample as \u201cpositive\u201d. By this way we do not need to collect negative samples, and the model is more general and the most important: explainable. The data intrinsic property to explore here is the class center for each positive class. The intuition is that if we can get the center of each class, then we can use the sample-center distance to reject or accept an sample as \u201cpositive\u201d. Now assume that the last layer of the neural network is a Dense module followed by a softmax activation which produces N class decisions. We\u2019ll refer the input of this Dense module as feature of the input sample (extracted by the former part of the whole neural network). For plain network trained with only positive samples, the feature distribution can be typically visualized as A Discriminative Deep Feature Learning Approach for Face Recognitions . Yandong Wen, Kaipeng Zhang, Zhifeng Li and Yu Qiao. European Conference on Computer Vision (ECCV) 2016 Center Loss Apparently the feature extracted by the plain model is not well centered, in other words, the feature distribution is not well-formed. Ideally, to reject or accept one sample as a certain class, we can set a probability threshold so that any sample whose feature satisfies \ud835\udc5d(\ud835\udc53_\ud835\udc57\u2502\ud835\udc36_\ud835\udc56)<\ud835\udc47_\ud835\udc56 \ud835\udc5d(\ud835\udc53_\ud835\udc57\u2502\ud835\udc36_\ud835\udc56)<\ud835\udc47_\ud835\udc56 will be rejected as an \u201coutsider\u201d for this class with certainty 1\u2212\ud835\udc47_\ud835\udc56 1\u2212\ud835\udc47_\ud835\udc56 But before we can do this, the distribution \ud835\udc5d(\ud835\udc53\u2502\ud835\udc36_\ud835\udc56) \ud835\udc5d(\ud835\udc53\u2502\ud835\udc36_\ud835\udc56) must be known. To get this conditional distribution, we can either traverse all the train samples and use any probability estimation / modelling method to approximate the true distribution, or we can resort to the DL method by directly requiring the neural network to produce features satisfying predefined distributions. The reason we can do this is because a neural network can be trained to emulate any nonlinear functions, and we can always transform a compact distribution into Gaussian by a certain function. To restrain the neural network to extract Gaussian distributed features, we assume each class has a mean feature vector (i.e., center) \ud835\udc53_{\ud835\udf07_\ud835\udc56} \ud835\udc53_{\ud835\udf07_\ud835\udc56} and require the model to minimize the distance between extracted feature and its corresponding center vector, i.e., min\u2061\u2016\ud835\udc53_\ud835\udc57\u2212\ud835\udc53_{\ud835\udf07_\ud835\udc56} \u2016^2 min\u2061\u2016\ud835\udc53_\ud835\udc57\u2212\ud835\udc53_{\ud835\udf07_\ud835\udc56} \u2016^2 \ud835\udc56\ud835\udc53 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 j j \ud835\udc4f\ud835\udc52\ud835\udc59\ud835\udc5c\ud835\udc5b\ud835\udc54\ud835\udc60 \ud835\udc61\ud835\udc5c \ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc56 \ud835\udc56 We refer this objective as \u201ccenter loss\u201d, the details can be found in Ref. [A Discriminative Deep Feature Learning Approach for Face Recognition. Yandong Wen, Kaipeng Zhang, Zhifeng Li and Yu Qiao. European Conference on Computer Vision (ECCV) 2016]. The model is trained now with both the categorical cross entropy loss and the center loss as min min \u2061 \ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc54\ud835\udc5c\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc36\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc66+\ud835\udf06\u2217\ud835\udc36\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc54\ud835\udc5c\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc36\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc66+\ud835\udf06\u2217\ud835\udc36\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 Center Module Now we\u2019ll go through the code part to illustrate how the center loss can be actually computed. To compute the center loss, we need first to get the center estimation of each class. This is done through a new module referred as Center . Check the code snippet following. class Center(Module): \"\"\" Compute the class centers during training Ref. to \"Discriminative feature learning approach for deep face recognition (2016)\" \"\"\" def __init__(self, feature_dim, center_num, alpha=0.9, center=init.GlorotUniform(), name=None): \"\"\" :param alpha: moving averaging coefficient :param center: initial value of center \"\"\" super().__init__(name=name) self.center = self.register_self_updating_variable(center, shape=[center_num, feature_dim], name=\"center\") self.alpha = alpha def forward(self, features, labels): \"\"\" :param features: (B, D) :param labels: (B,) :return: categorical centers \"\"\" center_batch = self.center[labels, :] diff = (self.alpha - 1.0) * (center_batch - features) center_updated = tensor.inc_subtensor(self.center[labels, :], diff) self.center.default_update = center_updated return self.center def predict(self): return self.center First, all our NN modules should subclass the root Module class, then we can use class methods and attributes to manipulate network parameters conveniently. Second, define the module initialization in .__init__() part. Here we do two things: we register a center tensor as network parameter and initialize it with a Glorot uniform random numpy array. The center tensor is of shape (center_num, feature_dim) , in which center_num should be equal to class number, and feature_dim is the dimension of extracted features by the network. In Dandelion, the network parameters are divided into two categories: 1) parameter to be updated by optimizer, 2) parameter to updated by user defined expression. The former parameters should be registered with class method .register_param() , and the latter parameters should be registered with class method . register_self_updating_variable() . Now we registered center tensor as self updating variable, its updating expression is given in .forward() function as self.center.update = center_updated . In Dandelion we use a specially named attribute . update to tell the framework that this parameter has an updating expression defined and the updating expression will be collected during Theano function compiling phase. The .forward() function will be used for training, and .predict() function will be used for inference. Basically, during training, the .forward() function computes moving averaging estimation of class centers; and during inference, we just use the stored center values as final estimated class centers. This is pretty much alike how BatchNorm \u2019s mean and std are estimated and used. Summary To summary, to write your own module, you only need to do the following three steps: 1) subclass Module class 2) register your module\u2019s parameters by .register_param() or . register_self_updating_variable() and initialize them 3) define the .forward() function for training and .predict() function for inference and that\u2019s it!","title":"II - Write Your Own Module"},{"location":"tutorial II - Write Your Own Module/#tutorial-ii-write-your-own-module","text":"In this tutorial, you\u2019ll learn how to write your own neural network module with the help of Dandelion. Here we\u2019ll design a module which gives the class centers for classification output. It\u2019s a simple case for Dandelion yet not so intuitive for Lasagne or Keras users. In image classification tasks, such as face recognition, document image classification, Imagenet contests, etc., we usually consider only the \u201cpositive\u201d samples, i.e., we assume that given any input sample, it would be associated with at least one out of all the known class labels. However, in actual applications, we often also want the trained neural network model to be able to tell whether an input sample is an \u201coutsider\u201d or not. To accomplish this task, we can add an extra \u201cnegative\u201d class to the final layer of the network, and then train this augmented network by feeding it with all kinds of \u201cnegative\u201d samples you can collect. It\u2019s pure data-driven, so the bottleneck is how many \u201cnegative\u201d samples can be collected. Another way is algorithm-driven: we design a new network module to explore the intrinsic properties of the data, and use these \u201cproperties\u201d to reject or accept an sample as \u201cpositive\u201d. By this way we do not need to collect negative samples, and the model is more general and the most important: explainable. The data intrinsic property to explore here is the class center for each positive class. The intuition is that if we can get the center of each class, then we can use the sample-center distance to reject or accept an sample as \u201cpositive\u201d. Now assume that the last layer of the neural network is a Dense module followed by a softmax activation which produces N class decisions. We\u2019ll refer the input of this Dense module as feature of the input sample (extracted by the former part of the whole neural network). For plain network trained with only positive samples, the feature distribution can be typically visualized as A Discriminative Deep Feature Learning Approach for Face Recognitions . Yandong Wen, Kaipeng Zhang, Zhifeng Li and Yu Qiao. European Conference on Computer Vision (ECCV) 2016","title":"Tutorial II: Write Your Own Module"},{"location":"tutorial II - Write Your Own Module/#center-loss","text":"Apparently the feature extracted by the plain model is not well centered, in other words, the feature distribution is not well-formed. Ideally, to reject or accept one sample as a certain class, we can set a probability threshold so that any sample whose feature satisfies \ud835\udc5d(\ud835\udc53_\ud835\udc57\u2502\ud835\udc36_\ud835\udc56)<\ud835\udc47_\ud835\udc56 \ud835\udc5d(\ud835\udc53_\ud835\udc57\u2502\ud835\udc36_\ud835\udc56)<\ud835\udc47_\ud835\udc56 will be rejected as an \u201coutsider\u201d for this class with certainty 1\u2212\ud835\udc47_\ud835\udc56 1\u2212\ud835\udc47_\ud835\udc56 But before we can do this, the distribution \ud835\udc5d(\ud835\udc53\u2502\ud835\udc36_\ud835\udc56) \ud835\udc5d(\ud835\udc53\u2502\ud835\udc36_\ud835\udc56) must be known. To get this conditional distribution, we can either traverse all the train samples and use any probability estimation / modelling method to approximate the true distribution, or we can resort to the DL method by directly requiring the neural network to produce features satisfying predefined distributions. The reason we can do this is because a neural network can be trained to emulate any nonlinear functions, and we can always transform a compact distribution into Gaussian by a certain function. To restrain the neural network to extract Gaussian distributed features, we assume each class has a mean feature vector (i.e., center) \ud835\udc53_{\ud835\udf07_\ud835\udc56} \ud835\udc53_{\ud835\udf07_\ud835\udc56} and require the model to minimize the distance between extracted feature and its corresponding center vector, i.e., min\u2061\u2016\ud835\udc53_\ud835\udc57\u2212\ud835\udc53_{\ud835\udf07_\ud835\udc56} \u2016^2 min\u2061\u2016\ud835\udc53_\ud835\udc57\u2212\ud835\udc53_{\ud835\udf07_\ud835\udc56} \u2016^2 \ud835\udc56\ud835\udc53 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 j j \ud835\udc4f\ud835\udc52\ud835\udc59\ud835\udc5c\ud835\udc5b\ud835\udc54\ud835\udc60 \ud835\udc61\ud835\udc5c \ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc56 \ud835\udc56 We refer this objective as \u201ccenter loss\u201d, the details can be found in Ref. [A Discriminative Deep Feature Learning Approach for Face Recognition. Yandong Wen, Kaipeng Zhang, Zhifeng Li and Yu Qiao. European Conference on Computer Vision (ECCV) 2016]. The model is trained now with both the categorical cross entropy loss and the center loss as min min \u2061 \ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc54\ud835\udc5c\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc36\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc66+\ud835\udf06\u2217\ud835\udc36\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc54\ud835\udc5c\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc36\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc66+\ud835\udf06\u2217\ud835\udc36\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60","title":"Center Loss"},{"location":"tutorial II - Write Your Own Module/#center-module","text":"Now we\u2019ll go through the code part to illustrate how the center loss can be actually computed. To compute the center loss, we need first to get the center estimation of each class. This is done through a new module referred as Center . Check the code snippet following. class Center(Module): \"\"\" Compute the class centers during training Ref. to \"Discriminative feature learning approach for deep face recognition (2016)\" \"\"\" def __init__(self, feature_dim, center_num, alpha=0.9, center=init.GlorotUniform(), name=None): \"\"\" :param alpha: moving averaging coefficient :param center: initial value of center \"\"\" super().__init__(name=name) self.center = self.register_self_updating_variable(center, shape=[center_num, feature_dim], name=\"center\") self.alpha = alpha def forward(self, features, labels): \"\"\" :param features: (B, D) :param labels: (B,) :return: categorical centers \"\"\" center_batch = self.center[labels, :] diff = (self.alpha - 1.0) * (center_batch - features) center_updated = tensor.inc_subtensor(self.center[labels, :], diff) self.center.default_update = center_updated return self.center def predict(self): return self.center First, all our NN modules should subclass the root Module class, then we can use class methods and attributes to manipulate network parameters conveniently. Second, define the module initialization in .__init__() part. Here we do two things: we register a center tensor as network parameter and initialize it with a Glorot uniform random numpy array. The center tensor is of shape (center_num, feature_dim) , in which center_num should be equal to class number, and feature_dim is the dimension of extracted features by the network. In Dandelion, the network parameters are divided into two categories: 1) parameter to be updated by optimizer, 2) parameter to updated by user defined expression. The former parameters should be registered with class method .register_param() , and the latter parameters should be registered with class method . register_self_updating_variable() . Now we registered center tensor as self updating variable, its updating expression is given in .forward() function as self.center.update = center_updated . In Dandelion we use a specially named attribute . update to tell the framework that this parameter has an updating expression defined and the updating expression will be collected during Theano function compiling phase. The .forward() function will be used for training, and .predict() function will be used for inference. Basically, during training, the .forward() function computes moving averaging estimation of class centers; and during inference, we just use the stored center values as final estimated class centers. This is pretty much alike how BatchNorm \u2019s mean and std are estimated and used.","title":"Center Module"},{"location":"tutorial II - Write Your Own Module/#summary","text":"To summary, to write your own module, you only need to do the following three steps: 1) subclass Module class 2) register your module\u2019s parameters by .register_param() or . register_self_updating_variable() and initialize them 3) define the .forward() function for training and .predict() function for inference and that\u2019s it!","title":"Summary"}]}