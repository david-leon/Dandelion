<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="David Leon (Dawei Leng)">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>dandelion.module - Dandelion</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "dandelion.module";
    var mkdocs_page_input_path = "dandelion_module.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Dandelion</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Tutorials</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../tutorial I - Sentence Topic Classification/">I - Sentence Topic Classification</a>
                </li>
                <li class="">
                    
    <a class="" href="../tutorial II - Write Your Own Module/">II - Write Your Own Module</a>
                </li>
                <li class="">
                    
    <a class="" href="../howtos/">III - Howtos</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Framework Interface</span>
    <ul class="subnav">
                <li class=" current">
                    
    <a class="current" href="./">dandelion.module</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#module">Module</a></li>
    

    <li class="toctree-l3"><a href="#dropout">Dropout</a></li>
    

    <li class="toctree-l3"><a href="#gru">GRU</a></li>
    

    <li class="toctree-l3"><a href="#lstm">LSTM</a></li>
    

    <li class="toctree-l3"><a href="#grucell">GRUCell</a></li>
    

    <li class="toctree-l3"><a href="#lstmcell">LSTMCell</a></li>
    

    <li class="toctree-l3"><a href="#conv2d">Conv2D</a></li>
    

    <li class="toctree-l3"><a href="#convtransposed2d">ConvTransposed2D</a></li>
    

    <li class="toctree-l3"><a href="#dense">Dense</a></li>
    

    <li class="toctree-l3"><a href="#embedding">Embedding</a></li>
    

    <li class="toctree-l3"><a href="#batchnorm">BatchNorm</a></li>
    

    <li class="toctree-l3"><a href="#center">Center</a></li>
    

    <li class="toctree-l3"><a href="#chaincrf">ChainCRF</a></li>
    

    <li class="toctree-l3"><a href="#sequential">Sequential</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_functional/">dandelion.functional</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_objective/">dandelion.objective</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_activation/">dandelion.activation</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_update/">dandelion.update</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_initialization/">dandelion.initialization</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_util/">dandelion.util</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_model/">dandelion.model</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Extensions</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../dandelion_ext_CV/">dandelion.ext.CV</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../history/">History</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Dandelion</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Framework Interface &raquo;</li>
        
      
    
    <li>dandelion.module</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="module">Module</h2>
<p>Root class of all network modules, you'd always subclass this for a new module</p>
<pre><code class="python">class Module(name=None, work_mode='inference')
</code></pre>

<ul>
<li><strong>name</strong>: module name, optional. If you don't specify the module name, it will be auto-named if this module is a sub-module of another module.</li>
<li><strong>work_mode</strong>: working mode, optional. Only used for the unified calling interface, check "Tutorial I" for detailed explanation.</li>
</ul>
<pre><code class="python">.params                  = []  
.self_updating_variables = [] 
.sub_modules             = OrderedDict()
.name                    = name
.work_mode               = work_mode
</code></pre>

<ul>
<li><strong>params</strong>: contains all the parameters which should be updated by optimizer (submodule excluded)</li>
<li><strong>self_updating_variables</strong>: contains all the parameters which are updated by user specified expression (submodule excluded)</li>
<li><strong>sub_modules</strong>: contains all the sub-modules</li>
</ul>
<pre><code class="python">.register_param(x, shape=None, name=None)
.register_self_updating_variable(x, shape=None, name=None)
</code></pre>

<p>Register and possibly initialize a parameter tensor. Parameters to be updated by optimizer should be registered with <code>register_param()</code> meanwhile parameters self-updated should be registerd with <code>register_self_updating_variable()</code></p>
<ul>
<li><strong>x</strong>: Theano shared variable, expression, numpy array or callable. Initial value, expression or initializer for this parameter.</li>
<li><strong>shape</strong>: tuple of int, optional. A tuple of integers representing the desired shape of the parameter tensor.</li>
<li><strong>name</strong>: str, optional. It's recommended to let the Dandelion framework name the variable automatically.</li>
</ul>
<pre><code class="python">.collect_params(include=None, exclude=None, include_self=True)
</code></pre>

<p>Collect parameters to be updated by optimizer.</p>
<ul>
<li><strong>include</strong>: sub-module keys, means which sub-module to include</li>
<li><strong>exclude</strong>: sub-module keys, means which sub-module to exclude</li>
<li><strong>include_self</strong>: whether include <code>self.params</code></li>
<li><strong>return</strong>: list of parameters, in the same order of sub-modules</li>
</ul>
<pre><code class="python">.collect_self_updates(include=None, exclude=None, include_self=True)
</code></pre>

<p>Collect all <code>update</code> from self_updating_variables.</p>
<ul>
<li><strong>include</strong>: sub-module keys, means which sub-module to include</li>
<li><strong>exclude</strong>: sub-module keys, means which sub-module to exclude</li>
<li><strong>include_self</strong>: whether include <code>self.self_updating_variables</code></li>
<li><strong>return</strong>: update dict, in the same order of sub-modules</li>
</ul>
<pre><code class="python">.get_weights()
</code></pre>

<p>Collect all module weights (including submodules)</p>
<ul>
<li><strong>return</strong>: list of tuples with format [variable.value, variable.name]</li>
</ul>
<pre><code class="python">.set_weights(module_weights, check_name='ignore')
</code></pre>

<p>Set module weights by default order (same order with <code>.get_weights()</code>)</p>
<ul>
<li><strong>module_weights</strong>: same with the return of <code>.get_weights()</code></li>
<li><strong>check_name</strong>: <code>ignore</code>|<code>warn</code>|<code>raise</code>. What to do if a weight's name does not match its corresponding variable's name.</li>
</ul>
<pre><code class="python">.set_weights_by_name(module_weights, unmatched='raise')
</code></pre>

<p>Set module weights by matching name.</p>
<ul>
<li><strong>module_weights</strong>: same with the return of <code>.get_weights()</code></li>
<li><strong>unmatched</strong>:  <code>ignore</code>|<code>warn</code>|<code>raise</code>. What to do if there remain weights or module variables unmatched.</li>
</ul>
<hr />
<h2 id="dropout">Dropout</h2>
<p>Sets values to zero with probability <code>p</code></p>
<pre><code class="python">class Dropout(seed=None, name=None)
</code></pre>

<ul>
<li><strong>seed</strong>: the random seed (integer) for initialization, optional</li>
</ul>
<pre><code class="python">.forward(input, p=0.5, shared_axes=(), rescale=True)
</code></pre>

<ul>
<li><strong>p</strong>: Ô¨Çoat or scalar tensor. The probability of setting a value to zero</li>
<li><strong>shared_axes</strong>: tuple of int. Axes to share the dropout mask over. By default, each value can be dropped individually. <code>shared_axes</code>=(0,) uses the same mask across the batch. <code>shared_axes</code>=(2, 3) uses the same mask across the spatial dimensions of 2D feature maps.</li>
<li><strong>rescale</strong>: bool. If True (the default), scale the input by 1 / (1 - <code>p</code>) when dropout is enabled, to keep the expected output mean the same.</li>
</ul>
<pre><code class="python">.predict( input, *args, **kwargs)
</code></pre>

<p>dummy interface, does nothing but returns the input unchanged.</p>
<p>Note: Theano uses <code>self_update</code> mechanism to implement pseudo randomness, so to use <code>Dropout</code> class, the followings are recommened:</p>
<ul>
<li>(1) define different instance for each droput layer</li>
<li>(2) compiling function with <code>no_default_updates=False</code></li>
</ul>
<hr />
<h2 id="gru">GRU</h2>
<p>Gated Recurrent Unit RNN.</p>
<pre><code class="python">class GRU(input_dims, hidden_dim, initializer=init.Normal(0.1), grad_clipping=0, 
          hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None)
</code></pre>

<ul>
<li><strong>input_dims</strong>: integer or list of integers. If scalar, input dimension = <code>input_dims</code>; if list of integers, input dimension = sum(<code>input_dims</code>), and GRU‚Äôs parameter <code>W_in</code> will be initialized unevenly by integers specified in input_dims</li>
<li><strong>hidden_dim</strong>: dimension of hidden units, also the output dimension</li>
<li><strong>grad_clipping</strong>: float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping</li>
<li><strong>hidden_activation</strong>: nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use <code>tanh</code> as default.</li>
<li><strong>learn_ini</strong>: whether learn initial state</li>
<li><strong>truncate_gradient</strong>: if not -1, BPTT will be used, gradient back-propagation will be performed at most <code>truncate_gradient</code> steps</li>
</ul>
<pre><code class="python">.forward(seq_input, h_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False)
</code></pre>

<ul>
<li><strong>seq_input</strong>: tensor with shape (T, B, D) in which D is the input dimension</li>
<li><strong>h_ini</strong>: initialization of hidden cell, (B, hidden_dim)</li>
<li><strong>seq_mask</strong>: mask for <code>seq_input</code></li>
<li><strong>backward</strong>: bool. Whether scan in backward direction</li>
<li><strong>only_return_final</strong>: bool. If <code>True</code>, only return the Ô¨Ånal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory.</li>
<li><strong>return_final_state</strong>: If <code>True</code>, the final state of <code>hidden</code> and <code>cell</code> will be returned, both (B, hidden_dim)</li>
</ul>
<pre><code class="python">.predict = .forward
</code></pre>

<hr />
<h2 id="lstm">LSTM</h2>
<p>Long Short-Term Memory RNN.<br />
The recurrent computation is implemented according to <a href="https://arxiv.org/abs/1308.0850">Graves' ‚ÄúGenerating sequences with recurrent neural networks.‚Äù</a>:</p>
<div>
<div class="MathJax_Preview">
i_t = \sigma_i(x_t W_{xi} + h_{t-1} W_{hi} + w_{ci} \odot c_{t-1} + b_i) \\
f_t = \sigma_f(x_t W_{xf} + h_{t-1} W_{hf} + w_{cf} \odot c_{t-1} + b_f) \\
c_t = f_t \odot c_{t - 1} + i_t \odot \sigma_c(x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\
o_t = \sigma_o(x_t W_{xo} + h_{t-1} W_{ho} + w_{co} \odot c_t + b_o) \\
h_t = o_t \odot \sigma_h(c_t)
</div>
<script type="math/tex; mode=display">
i_t = \sigma_i(x_t W_{xi} + h_{t-1} W_{hi} + w_{ci} \odot c_{t-1} + b_i) \\
f_t = \sigma_f(x_t W_{xf} + h_{t-1} W_{hf} + w_{cf} \odot c_{t-1} + b_f) \\
c_t = f_t \odot c_{t - 1} + i_t \odot \sigma_c(x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\
o_t = \sigma_o(x_t W_{xo} + h_{t-1} W_{ho} + w_{co} \odot c_t + b_o) \\
h_t = o_t \odot \sigma_h(c_t)
</script>
</div>
<pre><code class="python">class LSTM( input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, 
            hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None)
</code></pre>

<ul>
<li><strong>input_dims</strong>: integer or list of integers. If scalar, input dimension = <code>input_dims</code>; if list of integers, input dimension = sum(<code>input_dims</code>), and LSTM‚Äôs parameter <code>W_in</code> will be initialized unevenly by integers specified in input_dims</li>
<li><strong>hidden_dim</strong>: dimension of hidden units, also the output dimension</li>
<li><strong>peephole</strong>: bool. Whether add peephole connection.</li>
<li><strong>grad_clipping</strong>: float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping</li>
<li><strong>hidden_activation</strong>: nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use <code>tanh</code> as default.</li>
<li><strong>learn_ini</strong>: whether learn initial state</li>
<li><strong>truncate_gradient</strong>: if not -1, BPTT will be used, gradient back-propagation will be performed at most <code>truncate_gradient</code> steps</li>
</ul>
<pre><code class="python">.forward(seq_input, h_ini=None, c_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False)
</code></pre>

<ul>
<li><strong>seq_input</strong>: tensor with shape (T, B, D) in which D is the input dimension</li>
<li><strong>h_ini</strong>: initialization of hidden state, (B, hidden_dim)</li>
<li><strong>c_ini</strong>: initialization of cell state, (B, hidden_dim)</li>
<li><strong>seq_mask</strong>: mask for seq_input</li>
<li><strong>backward</strong>: bool. Whether scan in backward direction</li>
<li><strong>only_return_final</strong>: bool. If <code>True</code>, only return the Ô¨Ånal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory.</li>
<li><strong>return_final_state</strong>: If <code>True</code>, the final state of <code>hidden</code> and <code>cell</code> will be returned, both (B, hidden_dim)</li>
</ul>
<pre><code class="python">.predict = .forward
</code></pre>

<hr />
<h2 id="grucell">GRUCell</h2>
<p>Gated Recurrent Unit RNN Cell</p>
<pre><code class="python">class GRUCell(input_dims, hidden_dim, initializer=init.Normal(0.1), grad_clipping=0, 
              hidden_activation=tanh, name=None)
</code></pre>

<ul>
<li><strong>input_dims</strong>: integer or list of integers. If scalar, input dimension = <code>input_dims</code>; if list of integers, input dimension = sum(<code>input_dims</code>), and GRUCell‚Äôs parameter <code>W_in</code> will be initialized unevenly by integers specified in input_dims</li>
<li><strong>hidden_dim</strong>: dimension of hidden units, also the output dimension</li>
<li><strong>grad_clipping</strong>: float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping</li>
<li><strong>hidden_activation</strong>: nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use <code>tanh</code> as default</li>
</ul>
<pre><code class="python">.forward(input, h_pre, mask=None)
</code></pre>

<ul>
<li><strong>input</strong>: tensor with shape (B, D) in which D is the input dimension</li>
<li><strong>h_pre</strong>: initialization of hidden cell, (B, hidden_dim)</li>
<li><strong>mask</strong>: mask for <code>input</code></li>
</ul>
<pre><code class="python">.predict = .forward
</code></pre>

<hr />
<h2 id="lstmcell">LSTMCell</h2>
<p>Long Short-Term Memory RNN Cell</p>
<pre><code class="python">class LSTMCell(input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, 
               hidden_activation=tanh, name=None)
</code></pre>

<ul>
<li><strong>input_dims</strong>: integer or list of integers. If scalar, input dimension = <code>input_dims</code>; if list of integers, input dimension = sum(<code>input_dims</code>), and LSTM‚Äôs parameter <code>W_in</code> will be initialized unevenly by integers specified in input_dims</li>
<li><strong>hidden_dim</strong>: dimension of hidden units, also the output dimension</li>
<li><strong>peephole</strong>: bool. Whether add peephole connection.</li>
<li><strong>grad_clipping</strong>: float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping</li>
<li><strong>hidden_activation</strong>: nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use <code>tanh</code> as default.</li>
</ul>
<pre><code class="python">.forward(input, h_pre, c_pre, mask=None)
</code></pre>

<ul>
<li><strong>input</strong>: tensor with shape (B, D) in which D is the input dimension</li>
<li><strong>h_pre</strong>: initialization of hidden state, (B, hidden_dim)</li>
<li><strong>c_pre</strong>: initialization of cell state, (B, hidden_dim)</li>
<li><strong>mask</strong>: mask for <code>input</code></li>
</ul>
<pre><code class="python">.predict = .forward
</code></pre>

<hr />
<h2 id="conv2d">Conv2D</h2>
<p>Convolution 2D</p>
<pre><code class="python">class Conv2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), pad='valid', 
             dilation=(1,1), num_groups=1, W=init.GlorotUniform(), b=init.Constant(0.), 
             flip_filters=True, convOP=tensor.nnet.conv2d, input_shape=(None,None), untie_bias=False, name=None)
</code></pre>

<ul>
<li><strong>input_channels</strong>: int. Input shape of Conv2D module is (B, input_channels, H_in, W_in)</li>
<li><strong>out_channels</strong>: int. Output shape of Conv2D module is (B output_channels, H_out, W_out)</li>
<li><strong>kernel_size</strong>: int scalar or tuple of int. Convolution kernel size</li>
<li><strong>stride</strong>: Factor by which to subsample the output</li>
<li><strong>pad</strong>: <code>same</code>/<code>valid</code>/<code>full</code> or 2-element tuple of int. Control image border padding.</li>
<li><strong>dilation</strong>: factor by which to subsample (stride) the input.</li>
<li><strong>num_groups</strong>: Divides the image, kernel and output tensors into num_groups separate groups. Each which carry out convolutions separately</li>
<li><strong>W</strong>: initialization of filter bank, shape = (out_channels, in_channels, kernel_size[0], kernel_size[1])</li>
<li><strong>b</strong>: initialization of convolution bias, shape = (out_channels,) if untie_bias is False; otherwise shape = (out_channels, H_out, W_out)</li>
<li><strong>flip_filters</strong>: If <code>True</code>, will flip the filter rows and columns before sliding them over the input. This operation is normally referred to as a convolution, and this is the default. If <code>False</code>, the filters are not flipped and the operation is referred to as a cross-correlation.</li>
<li><strong>input_shape</strong>: optional, (H_in, W_in)</li>
<li><strong>untie_bias</strong>: If <code>False</code>, the module will have a bias parameter for each channel, which is shared across all positions in this channel. As a result, the b attribute will be a vector (1D). If <code>True</code>, the module will have separate bias parameters for each position in each channel. As a result, the b attribute will be a 3D tensor.</li>
</ul>
<hr />
<h2 id="convtransposed2d">ConvTransposed2D</h2>
<p>Transposed convolution 2D. Also known as fractionally-strided convolution or deconvolution (although it is not an actual deconvolution operation)</p>
<pre><code class="python">class ConvTransposed2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), pad='valid', 
                       dilation=(1,1), num_groups=1, W=init.GlorotUniform(), b=init.Constant(0.), 
                       flip_filters=True, input_shape=(None,None), untie_bias=False, name=None)
</code></pre>

<ul>
<li><strong>return</strong>: output shape = <code>(B, C, H, W)</code>, in which <code>H = ((H_in - 1) * stride_H) + kernel_H - 2 * pad_H</code>, and the same with <code>W</code>.</li>
</ul>
<p>All the parameters have the same meanings with <code>Conv2D</code> module. In fact, the transposed convolution is equal to upsampling the input then doing conventional convolution. However, for efficiency purpose, here the transposed convolution is implemented via Theano‚Äôs <code>AbstractConv2d_gradInputs</code> as what is done in Lasagne.</p>
<hr />
<h2 id="dense">Dense</h2>
<p>Fully connected network, also known as affine transform. Apply affine transform <code>Wx+b</code> to the last dimension of input.<br />
The input of <code>Dense</code> can have any dimensions, and note that we do not apply any activation to its output by default</p>
<pre><code class="python">class Dense(input_dims, output_dim, W=init.GlorotUniform(), b=init.Constant(0.), name=None)
</code></pre>

<ul>
<li><strong>input_dims</strong>: integer or list of integers. If scalar, input dimension = input_dims; if list of integers, input dimension = sum(input_dims), and Dense‚Äôs parameter <code>W</code> will be initialized unevenly by integers specified in input_dims</li>
<li><strong>output_dim</strong>: output dimension</li>
<li><strong>W</strong>, <strong>b</strong>: parameter initialization</li>
</ul>
<hr />
<h2 id="embedding">Embedding</h2>
<p>Word/character embedding module.</p>
<pre><code class="python">class Embedding(num_embeddings, embedding_dim, W=init.Normal(), name=None)
</code></pre>

<ul>
<li><strong>num_embeddings</strong>: the Number of different embeddings</li>
<li><strong>embedding_dim</strong>: output embedding vector dimension</li>
</ul>
<hr />
<h2 id="batchnorm">BatchNorm</h2>
<p>Batch normalization module.</p>
<pre><code class="python">class BatchNorm(input_shape=None, axes='auto', eps=1e-4, alpha=0.1, beta=init.Constant(0), gamma=init.Constant(1), 
                mean=init.Constant(0), inv_std=init.Constant(1), mode='high_mem', name=None)
</code></pre>

<ul>
<li><strong>input_shape</strong>: Tuple or list of ints or tensor variables. Input shape of <code>BatchNorm</code> module, including batch dimension. </li>
<li><strong>axes</strong>: <code>auto</code> or tuple of int. The axis or axes to normalize over. If <code>auto</code> (the default), normalize over all axes except for the second: this will normalize over the minibatch dimension for dense layers, and additionally over all spatial dimensions for convolutional layers.</li>
<li><strong>eps</strong>: Small constant ùúñ added to the variance before taking the square root and dividing by it, to avoid numerical problems</li>
<li><strong>alpha</strong>: Coefficient for the exponential moving average of batch-wise means and standard deviations computed during training; the closer to one, the more it will depend on the last batches seen</li>
<li><strong>mode</strong>: <code>low_mem</code> or <code>high_mem</code>. Specify which batch normalization implementation that will be used. As no intermediate representations are stored for the back-propagation, <code>low_mem</code> implementation lower the memory usage, however, it is 5-10% slower than <code>high_mem</code> implementation. Note that 5-10% computation time difference compare the batch normalization operation only, time difference between implementation is likely to be less important on the full model fprop/bprop.</li>
</ul>
<pre><code class="python">.forward(input, use_input_mean=True)
</code></pre>

<ul>
<li><strong>use_input_mean</strong>: default, use mean &amp; std of input batch for normalization; if <code>False</code>, <code>self.mean</code> and <code>self.std</code> will be used for normalization. The reason that input mean is used during training is because at the early training stage, <code>BatchNorm</code>'s <code>self.mean</code> is far from the expected mean value and can be detrimental for network convergence. It's recommended to use input mean for early stage training; after that, you can switch to <code>BatchNorm</code>'s <code>self.mean</code> for training &amp; inference consistency.</li>
</ul>
<hr />
<h2 id="center">Center</h2>
<p>Estimate class centers by moving averaging.</p>
<pre><code class="python">class Center(feature_dim, center_num, center=init.GlorotUniform(), name=None)
</code></pre>

<ul>
<li><strong>feature_dim</strong>: feature dimension </li>
<li><strong>center_num</strong>: class center number</li>
<li><strong>center</strong>: initialization of class centers, should be in shape of <code>(center_num, feature_dim)</code></li>
</ul>
<pre><code class="python">.forward(features, labels, alpha=0.1)
</code></pre>

<ul>
<li><strong>features</strong>: batch features, from which the class centers will be estimated</li>
<li><strong>labels</strong>: <code>features</code>'s corresponding class labels</li>
<li><strong>alpha</strong>: moving averaging coefficient, the closer to one, the more it will depend on the last batches seen: <span><span class="MathJax_Preview">C_{new} = \alpha*C_{batch} + (1-\alpha)*C_{old}</span><script type="math/tex">C_{new} = \alpha*C_{batch} + (1-\alpha)*C_{old}</script></span></li>
<li><strong>return</strong>: centers estimated</li>
</ul>
<pre><code class="python">.predict()
</code></pre>

<ul>
<li><strong>return</strong>: centers stored</li>
</ul>
<hr />
<h2 id="chaincrf">ChainCRF</h2>
<p>Linear chain CRF layer for sequence labeling.</p>
<pre><code class="python">class ChainCRF(state_num, transitions=init.GlorotUniform(), p_scale=1.0, l1_regularization=0.001, 
               state_pad=True, transition_matrix_normalization=True,  name=None)
</code></pre>

<ul>
<li><strong>state_num</strong>: number of hidden states. If <code>state_pad</code> is <code>True</code>, then the actual state number inside CRF will be <code>state_num + 2</code>.</li>
<li><strong>transitions</strong>: initialization of transition matrix, in shape of <code>(state_num+2, state_num+2)</code> if <code>state_pad</code> is <code>True</code>, else <code>(state_num, state_num)</code></li>
<li><strong>p_scale</strong>: probability scale factor. The input of this module will be multiplied by this factor.</li>
<li><strong>l1_regularization</strong>: L1 regularization coefficient for <code>transition</code> matrix</li>
<li><strong>state_pad</strong>: whether do state padding. CRF requires two additional dummy states, i.e., <code>&lt;bos&gt;</code> and <code>&lt;eos&gt;</code> (beginning and endding of sequence). The <code>ChainCRF</code> module can pad the state automatically with these two dummy states, or you can incorporate these two states in module input. In the latter case, set <code>state_pad</code> to <code>False</code>.</li>
<li><strong>transition_matrix_normalization</strong>: whether do row-wise normalization of transition matrix. You may expect that each row of the <code>transition</code> matrix should sum to 1.0, and to do this, set this flag to <code>True</code>.</li>
</ul>
<pre><code class="python">.forward(x, y)
</code></pre>

<p>Compute CRF loss</p>
<ul>
<li><strong>x</strong>: output from previous RNN layer, in shape of (B, T, N)</li>
<li><strong>y</strong>: tag ground truth, in shape of (B, T), int32</li>
<li><strong>return</strong>: loss in shape of (B,) if <code>l1_regularization</code> disabled, else in shape of (1,)</li>
</ul>
<pre><code class="python">.predict(x)
</code></pre>

<p>CRF Viterbi decoding</p>
<ul>
<li><strong>x</strong>: output from previous RNN layer, in shape of (B, T, N)</li>
<li><strong>return</strong>: decoded sequence</li>
</ul>
<hr />
<h2 id="sequential">Sequential</h2>
<p>Sequential container for a list of modules, just for convenience.</p>
<pre><code class="python">class Sequential(module_list, activation=linear, name=None)
</code></pre>

<ul>
<li><strong>module_list</strong>: list of network sub-modules, these modules <strong>MUST NOT</strong> be sub-modules of any other parent module.</li>
<li><strong>activation</strong>: activation applied to output of each sub-module.</li>
</ul>
<pre><code class="python">.forward(x)
</code></pre>

<p>Forward pass through the network module sequence.</p>
<pre><code class="python">.predict(x)
</code></pre>

<p>Inference pass through the network module sequence.</p>
<p>Example:</p>
<pre><code class="python">        conv1 = Conv2D(in_channels=1, out_channels=3, stride=(2,2))
        bn1   = BatchNorm(input_shape=(None, 3, None, None))
        conv2 = Conv2D(in_channels=3, out_channels=5)
        conv3 = Conv2D(in_channels=5, out_channels=8)
        model = Sequential([conv1, bn1, conv2, conv3], activation=relu, name='Seq')

        x = tensor.ftensor4('x')
        y = model.forward(x)
        print('compiling fn...')
        fn = theano.function([x], y, no_default_updates=False)
        print('run fn...')
        input = np.random.rand(4, 1, 32, 33).astype(np.float32)
        output = fn(input)
        print(output)
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../dandelion_functional/" class="btn btn-neutral float-right" title="dandelion.functional">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../howtos/" class="btn btn-neutral" title="III - Howtos"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../howtos/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../dandelion_functional/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
