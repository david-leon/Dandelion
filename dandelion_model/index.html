<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="David Leon (Dawei Leng)">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>dandelion.model - Dandelion</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "dandelion.model";
    var mkdocs_page_input_path = "dandelion_model.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Dandelion</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Tutorials</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../tutorial I - Sentence Topic Classification/">I - Sentence Topic Classification</a>
                </li>
                <li class="">
                    
    <a class="" href="../tutorial II - Write Your Own Module/">II - Write Your Own Module</a>
                </li>
                <li class="">
                    
    <a class="" href="../howtos/">III - Howtos</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Framework Interface</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../dandelion_module/">dandelion.module</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_functional/">dandelion.functional</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_activation/">dandelion.activation</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_objective/">dandelion.objective</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_update/">dandelion.update</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_initialization/">dandelion.initialization</a>
                </li>
                <li class="">
                    
    <a class="" href="../dandelion_util/">dandelion.util</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">dandelion.model</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#vgg-16-network">VGG-16 network</a></li>
    

    <li class="toctree-l3"><a href="#depthwise-separable-convolution">Depthwise Separable Convolution</a></li>
    

    <li class="toctree-l3"><a href="#resnet-bottleneck">ResNet bottleneck</a></li>
    

    <li class="toctree-l3"><a href="#feature-pyramid-network">Feature Pyramid Network</a></li>
    

    <li class="toctree-l3"><a href="#shuffleunit">ShuffleUnit</a></li>
    

    <li class="toctree-l3"><a href="#shuffleunit_stack">ShuffleUnit_Stack</a></li>
    

    <li class="toctree-l3"><a href="#shufflenet">ShuffleNet</a></li>
    

    <li class="toctree-l3"><a href="#shuffleunit_v2">ShuffleUnit_v2</a></li>
    

    <li class="toctree-l3"><a href="#shuffleunit_v2_stack">ShuffleUnit_v2_Stack</a></li>
    

    <li class="toctree-l3"><a href="#shufflenet_v2">ShuffleNet_v2</a></li>
    

    <li class="toctree-l3"><a href="#ctpn">CTPN</a></li>
    

    <li class="toctree-l3"><a href="#u-net-fcn">U-net FCN</a></li>
    

    <li class="toctree-l3"><a href="#shuffle-seg-network">Shuffle-Seg network</a></li>
    

    <li class="toctree-l3"><a href="#alternate-2d-lstm">Alternate 2D LSTM</a></li>
    

    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Extensions</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../dandelion_ext_CV/">dandelion.ext.CV</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../history/">History</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Dandelion</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Framework Interface &raquo;</li>
        
      
    
    <li>dandelion.model</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="vgg-16-network">VGG-16 network</h2>
<p>Reference implementation of the classic <a href="https://arxiv.org/abs/1409.1556">VGG-16</a> network</p>
<pre><code class="python">class model_VGG16(channel=3, im_height=224, im_width=224, Nclass=1000, 
                  kernel_size=3, border_mode=(1, 1), flip_filters=False)
</code></pre>

<ul>
<li><strong>channel</strong>: input channel number</li>
<li><strong>Nclass</strong>: output class number</li>
</ul>
<p>The model accepts input of shape in the order of (B, C, H, W), and outputs with shape (B, N).</p>
<hr />
<h2 id="depthwise-separable-convolution">Depthwise Separable Convolution</h2>
<p>Reference implementation of <a href="https://arxiv.org/abs/1610.02357">Depthwise Separable Convolution</a></p>
<pre><code class="python">class DSConv2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), 
               dilation=(1,1), pad='valid')
</code></pre>

<ul>
<li><strong>input_channels</strong>: int. Input shape is (B, input_channels, H_in, W_in)</li>
<li><strong>out_channels</strong>: int. Output shape is (B output_channels, H_out, W_out)</li>
<li><strong>kernel_size</strong>: int scalar or tuple of int. Convolution kernel size</li>
<li><strong>stride</strong>: Factor by which to subsample the output</li>
<li><strong>pad</strong>: <code>same</code>/<code>valid</code>/<code>full</code> or 2-element tuple of int. Control image border padding.</li>
<li><strong>dilation</strong>: factor by which to subsample (stride) the input.</li>
</ul>
<p>The model do the depthwise 2D convolution per-channel of input, then map the output to #out_channels number of channel by pointwise 1*1 convolution. No activation applied inside.</p>
<hr />
<h2 id="resnet-bottleneck">ResNet bottleneck</h2>
<p>Reference implementation of bottleneck building block of <a href="https://arxiv.org/abs/1512.03385">ResNet</a> network</p>
<pre><code class="python">class ResNet_bottleneck(outer_channel=256, inner_channel=64, border_mode='same',
                        batchnorm_mode=1, activation=relu)
</code></pre>

<ul>
<li><strong>outer_channel</strong>: channel number of block input</li>
<li><strong>inner_channel</strong>: channel number inside the block</li>
<li><strong>batchnorm_mode</strong>: {0 | <em>1</em> | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn</li>
<li><strong>activation</strong>: default = relu. <strong>Note no activation applied to the last element-wise sum output.</strong></li>
</ul>
<p>The model accepts input of shape in the order of (B, C, H, W), and outputs with the same shape.</p>
<hr />
<h2 id="feature-pyramid-network">Feature Pyramid Network</h2>
<p>Reference implementation of <a href="https://arxiv.org/abs/1612.03144">feature pyramid network</a></p>
<pre><code class="python">class model_FPN(input_channel=3, base_n_filters=64, batchnorm_mode=1)
</code></pre>

<ul>
<li><strong>batchnorm_mode</strong>: same with <code>ResNet_bottleneck</code></li>
<li><strong>return</strong> 4-element tuple <code>(p2, p3, p4, p5)</code>,  CNN pyramid features at different scales, each with #channel = 4 * <code>base_n_filters</code></li>
</ul>
<hr />
<h2 id="shuffleunit">ShuffleUnit</h2>
<p>Reference implementation of <a href="https://arxiv.org/abs/1707.01083">shuffle-net</a> unit</p>
<pre><code class="python">class ShuffleUnit(in_channels=256, inner_channels=None, out_channels=None, group_num=4, border_mode='same', 
                  batchnorm_mode=1, activation=relu, stride=(1,1), dilation=(1,1), fusion_mode='add')
</code></pre>

<ul>
<li><strong>in_channels</strong>: channel number of unit input</li>
<li><strong>inner_channel</strong>: optional, channel number inside the unit, default = <code>in_channels//4</code></li>
<li><strong>out_channels</strong>: channel number of unit output, only used when <code>fusion_mode</code> = 'concat', and must &gt; <code>in_channels</code></li>
<li><strong>group_num</strong>: number of convolution groups</li>
<li><strong>border_mode</strong>: only <code>same</code> allowed</li>
<li><strong>batchnorm_mode</strong>: {0 | <em>1</em> | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn</li>
<li><strong>activation</strong>: default = relu. <strong>Note no activation applied to the last output.</strong></li>
<li><strong>stride, dilation</strong>: only used for depthwise separable convolution module inside</li>
<li><strong>fusion_mode</strong>: {'add' | 'concat'}.</li>
<li><strong>return</strong>: convolution result with #channel = <code>in_channels</code> when <code>fusion_mode</code>='add', #channel = <code>out_channels</code> when <code>fusion_mode</code>='concat'</li>
</ul>
<h2 id="shuffleunit_stack">ShuffleUnit_Stack</h2>
<p>Reference implementation of shuffle-net unit stack</p>
<pre><code class="python">class ShuffleUnit_Stack(in_channels, inner_channels=None, out_channels=None, group_num=4, batchnorm_mode=1, 
                        activation=relu, stack_size=3)
</code></pre>

<ul>
<li><strong>in_channels</strong>: channel number of input</li>
<li><strong>inner_channel</strong>: optional, channel number inside the shuffle-unit, default = <code>in_channels//4</code></li>
<li><strong>out_channels</strong>: channel number of stack output, must &gt; <code>in_channels</code></li>
<li><strong>group_num</strong>: number of convolution groups</li>
<li><strong>batchnorm_mode</strong>: {0 | <em>1</em> | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn</li>
<li><strong>activation</strong>: default = relu. <strong>Note no activation applied to the last output.</strong></li>
<li><strong>stack_size</strong>: number of shuffle-unit in the stack</li>
</ul>
<h2 id="shufflenet">ShuffleNet</h2>
<p>Reference implementation of <a href="https://arxiv.org/abs/1707.01083">shuffle-net</a>, without the final pooling &amp; Dense layer.</p>
<pre><code class="python">class model_ShuffleNet(in_channels, group_num=4, stage_channels=(24, 272, 544, 1088), stack_size=(3, 7, 3), 
                       batchnorm_mode=1, activation=relu)
</code></pre>

<ul>
<li><strong>in_channels</strong>: channel number of input</li>
<li><strong>group_num</strong>: number of convolution groups</li>
<li><strong>stage_channels</strong>: channel number of each stage output.</li>
<li><strong>stack_size</strong>: size of each stack.</li>
<li><strong>batchnorm_mode</strong>: {0 | <em>1</em> | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn</li>
<li><strong>activation</strong>: default = relu. <strong>Note no activation applied to the last output.</strong></li>
</ul>
<hr />
<h2 id="shuffleunit_v2">ShuffleUnit_v2</h2>
<p>Reference implementation of <a href="https://arxiv.org/abs/1807.11164">shufflenet_v2</a> unit</p>
<pre><code class="python">class ShuffleUnit_v2(in_channels=256, out_channels=None, border_mode='same', batchnorm_mode=1, 
                     activation=relu, stride=1, dilation=1)
</code></pre>

<ul>
<li><strong>in_channels</strong>: channel number of unit input</li>
<li><strong>out_channels</strong>: channel number of unit output, only used when <code>fusion_mode</code> = 'concat', and must &gt; <code>in_channels</code></li>
<li><strong>border_mode</strong>: only <code>same</code> allowed</li>
<li><strong>batchnorm_mode</strong>: {0 | <em>1</em> | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn</li>
<li><strong>activation</strong>: default = relu. <strong>Note no activation applied to the last output.</strong></li>
<li><strong>stride, dilation</strong>: only used for depthwise separable convolution module inside, must be integer scalars</li>
</ul>
<h2 id="shuffleunit_v2_stack">ShuffleUnit_v2_Stack</h2>
<p>Reference implementation of shufflenet_v2 unit stack</p>
<pre><code class="python">class ShuffleUnit_v2_Stack(in_channels, out_channels, batchnorm_mode=1, activation=relu, stack_size=3)
</code></pre>

<ul>
<li><strong>in_channels</strong>: channel number of input</li>
<li><strong>out_channels</strong>: channel number of stack output</li>
<li><strong>batchnorm_mode</strong>: {0 | <em>1</em> | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn</li>
<li><strong>activation</strong>: default = relu. <strong>Note no activation applied to the last output.</strong></li>
<li><strong>stack_size</strong>: number of shuffle-unit in the stack</li>
</ul>
<h2 id="shufflenet_v2">ShuffleNet_v2</h2>
<p>Reference implementation of <a href="https://arxiv.org/abs/1807.11164">shufflenet_v2</a>, without the final pooling &amp; Dense layer.</p>
<pre><code class="python">class model_ShuffleNet_v2(in_channels, stage_channels=(24, 116, 232, 464, 1024), stack_size=(3, 7, 3), 
                          batchnorm_mode=1, activation=relu)
</code></pre>

<ul>
<li><strong>in_channels</strong>: channel number of input</li>
<li><strong>stage_channels</strong>: channel number of each stage output.</li>
<li><strong>stack_size</strong>: size of each stack.</li>
<li><strong>batchnorm_mode</strong>: {0 | <em>1</em> | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn</li>
<li><strong>activation</strong>: default = relu. <strong>Note no activation applied to the last output.</strong></li>
</ul>
<hr />
<h2 id="ctpn">CTPN</h2>
<p>Model reference implementation of <a href="https://arxiv.org/abs/1609.03605">CTPN</a></p>
<pre><code class="python">class model_CTPN(k=10, do_side_refinement_regress=False,
                 batchnorm_mode=1, channel=3, im_height=None, im_width=None,
                 kernel_size=3, border_mode=(1, 1), VGG_flip_filters=False,
                 im2col=None)
</code></pre>

<ul>
<li><strong>k</strong>: anchor box number</li>
<li><strong>do_side_refinement_regress</strong>: whether implement side refinement regression</li>
<li><strong>batchnorm_mode</strong>: {0|<em>1</em>}, whether insert batch normalization into the end of each convolution stage of VGG-16 net, useful for cold start.</li>
<li><strong>channel</strong>: input channel number</li>
<li><strong>im_height, im_width</strong>: input image height/width, optional</li>
<li><strong>kernel_size</strong>: convolution kernel size of VGG-16 net</li>
<li><strong>border_mode</strong>: border mode of VGG-16 net</li>
<li><strong>VGG_flip_filters</strong>: whether flip convolution kernels for VGG-16 net</li>
<li><strong>im2col</strong>: function corresponding to Caffe's <code>im2col()</code>. If <code>None</code>, the CTPN implementation will not strictly follow the original paper.</li>
</ul>
<hr />
<h2 id="u-net-fcn">U-net FCN</h2>
<p>Reference implementation of <a href="https://arxiv.org/abs/1505.04597">U-net</a> FCN</p>
<pre><code class="python">class model_Unet(channel=1, im_height=128, im_width=128, Nclass=2, kernel_size=3, 
                 border_mode='same', base_n_filters=64, output_activation=softmax)
</code></pre>

<ul>
<li><strong>channel</strong>: input channel number</li>
<li><strong>Nclass</strong>: output channel number</li>
</ul>
<p>The model accepts input of shape in the order of (B, C, H, W), and outputs with shape in the order of (B, H, W, C).</p>
<hr />
<h2 id="shuffle-seg-network">Shuffle-Seg network</h2>
<p>Model reference implementation of <a href="https://arxiv.org/abs/1803.03816">ShuffleSeg</a></p>
<pre><code class="python">class model_ShuffleSeg(in_channels=1, Nclass=6, SF_group_num=4, SF_stage_channels=(24, 272, 544, 1088), 
                       SF_stack_size=(3, 7, 3), SF_batchnorm_mode=1, SF_activation=relu)
</code></pre>

<ul>
<li><strong>in_channels</strong>: channel number of input</li>
<li><strong>Nclass</strong>: output class number</li>
<li><strong>SF_group_num</strong>: number of convolution groups for inside ShuffleNet encoder.</li>
<li><strong>SF_stage_channels</strong>: channel number of each stage output for inside ShuffleNet encoder.</li>
<li><strong>SF_stack_size</strong>: size of each stack for inside ShuffleNet encoder.</li>
<li><strong>SF_batchnorm_mode</strong>: {0 | <em>1</em> | 2}. 0 means no batch normalization applied; 1 means batch normalization applied to each cnn; 2 means batch normalization only applied to the last cnn. For inside ShuffleNet encoder</li>
<li><strong>SF_activation</strong>: default = relu. For inside ShuffleNet encoder.</li>
</ul>
<hr />
<h2 id="alternate-2d-lstm">Alternate 2D LSTM</h2>
<p>LSTM2D implementation by alternating LSTM along different dimensions.<br />
Input shape = <code>(H, W, B, C)</code></p>
<pre><code class="python">class Alternate_2D_LSTM( input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, 
                         hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, mode=2)
</code></pre>

<p>All the arguments are the same with <code>LSTM</code> module, except for <code>mode</code>.</p>
<ul>
<li><strong>mode</strong>: {0 | 1 | <em>2</em>}. <br />
0: concat mode, 1D LSTM results from horizontal and vertical dimensions are concatenated along the <code>C</code> dimension, i.e.,<br />
<span><span class="MathJax_Preview">result = concat(horizontal\_LSTM(input), vertical\_LSTM(input))</span><script type="math/tex">result = concat(horizontal\_LSTM(input), vertical\_LSTM(input))</script></span>; <br />
1: sequential mode, horizontal and vertical dimensions are processed sequentially, i.e., <span><span class="MathJax_Preview">result = horizontal\_LSTM(vertical\_LSTM(input))</span><script type="math/tex">result = horizontal\_LSTM(vertical\_LSTM(input))</script></span>; <br />
2: mixed mode, i.e.,<br />
<span><span class="MathJax_Preview">result = horizontal\_LSTM(concat(input, vertical\_LSTM(input)))</span><script type="math/tex">result = horizontal\_LSTM(concat(input, vertical\_LSTM(input)))</script></span></li>
</ul>
<pre><code class="python">.forward(seq_input, h_ini=(None, None), c_ini=(None, None), seq_mask=None, backward=(False, False), return_final_state=False)
</code></pre>

<p>All the arguments are the same with <code>LSTM</code> module</p>
<pre><code class="python">.predict = .forward
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../dandelion_ext_CV/" class="btn btn-neutral float-right" title="dandelion.ext.CV">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../dandelion_util/" class="btn btn-neutral" title="dandelion.util"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../dandelion_util/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../dandelion_ext_CV/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
